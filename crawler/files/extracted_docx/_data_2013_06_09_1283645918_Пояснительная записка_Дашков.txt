Оглавление


































































































































































Введение

Актуальность проблемы обусловлена тем, что в настоящее время проблема расширения сетевой инфраструктуры компаний требует отдельного внимания. Компании часто расширяются, реструктурируются, меняют сферу деятельности, или, наоборот, сокращаются, что приводит к необходимости динамически изменять и сетевую инфраструктуру. Данные изменения в итоге ведут к дополнительным затратам. Например, при необходимости создания дополнительных рабочих мест, придется покупать новое оборудование. В зависимости от нужд предприятия, затраты на новое оборудования могут быть как относительно небольшими, так и достигать огромных пределов. В случае, если проводится сокращение, или определенный проект закрывается, оборудование использоваться не будет, хотя затраты на его содержание и обслуживание останутся. В последнее время получили широкое распространение «облачные» технологии.[12, 49] Они подразумевают под собой быстрый доступ по требованию к вычислительным ресурсам. Покупатель платит «по факту» использования ресурсов, что является весьма удобным решением проблемы, описанной выше. Причем ресурсы, предоставляемые клиенту, являются масштабируемыми, что позволяет увеличить мощность используемой системы без покупки нового оборудования. Расходы на обслуживание «виртуального сервера» значительно меньше расходов на обслуживание физического. В облаках обеспечена наивысшая безопасность работы с данными, что в свою очередь уменьшает расходы на защиту информации в сетях компании. Дополнительным плюсом можно считать то, что мониторингом и обслуживанием виртуальной сети в облаке может заниматься меньше человек, чем в физической. Если раньше многие компании содержали целый ИТ–отдел в штате сотрудников, то теперь достаточно несколько опытных человек, которые будут отвечать за данную деятельность.

Огромным достоинством облачных технологий является их высокая доступность. Сейчас существует множество предложений по построению облачной сетевой инфраструктуры как для бизнеса, так и для научной деятельности. Мощность оборудования, на котором работают облачные сервисы, огромна. Это позволяет поставщикам облачных сервисов предоставлять вычислительные ресурсы клиентам с любыми потребностями.

В то же время, одно из основных требований к любой системе – это высокая  эффективность. Грамотная организация работы гипервизора – элемента, отвечающего за управление виртуальными машинами в облаке, позволяет существенно повысить эффективность облачной системы.




Глава 1. Облачные системы



Определение

Облачные вычисления(Cloud computing) – это модель, обеспечивающая удобный сетевой доступ по требованию к общим конфигурируемым вычислительным ресурсам (сетям, серверам, хранилищам данных, приложениям и сервисам), который оперативно предоставляется с минимальными усилиями по управлению и взаимодействию с сервис-провайдером. Это определение Национального института стандартов (NIST) широко распространено во всей отрасли [12,49].



Характеристики облачных систем

Облачные вычисления строятся на фундаменте таких технологий, как распределённые вычисления, включающие в себя кластеризацию, серверную виртуализацию и динамическое предоставление ресурсов, а также сервисы общего пользования SOA и широкомасштабную автоматизацию управления.

Пять основных характеристик облаков [12,46,49].

• Самообслуживание по требованию – Пользователи способны получать, контролировать и управлять вычислительными ресурсами без помощи системных администраторов

• Широкий сетевой доступ – Вычислительные сервисы предоставляются через стандартные сети и гетерогенные устройства, вне зависимости от используемого терминального устройства

• Оперативная эластичность – IT-ресурсы могут оперативно масштабироваться (увеличиваться или уменьшаться)  в любую сторону по мере надобности

• Пул ресурсов – поставщик услуг объединяет ресурсы для обслуживания большого числа потребителей в единый пул для динамического перераспределения мощностей между потребителями в условиях постоянного изменения спроса на мощности; при этом потребители контролируют только основные параметры услуги (например, объём данных, скорость доступа), но фактическое распределение ресурсов, предоставляемых потребителю, осуществляет поставщик (в некоторых случаях потребители всё-таки могут управлять некоторыми физическими параметрами перераспределения, например, указывать желаемый центр обработки данных из соображений географической близости);




• Расчёт стоимости услуги – Использование IT-ресурса отслеживается по каждому приложению и пользователю



Модели облачных вычислений



Три сервисных модели: [12,41,49]

Программное обеспечение как услуга

Программное обеспечение как услуга (SaaS, англ. Software-as-a-Service) — модель, в которой потребителю предоставляется возможность использования прикладного программного обеспечения провайдера, работающего в облачной инфраструктуре и доступного из различных клиентских устройств или посредством тонкого клиента, например, из браузера (например, веб-почта) или интерфейс программы. Контроль и управление основной физической и виртуальной инфраструктурой облака, в том числе сети, серверов, операционных систем, хранения, или даже индивидуальных возможностей приложения (за исключением ограниченного набора пользовательских настроек конфигурации приложения) осуществляется облачным провайдером.

Платформа как услуга

Платформа как услуга (PaaS, англ. Platform-as-a-Service) — модель, когда потребителю предоставляется возможность использования облачной инфраструктуры для размещения базового программного обеспечения для последующего размещения на нём новых или существующих приложений (собственных, разработанных на заказ или приобретённых тиражируемых приложений). В состав таких платформ входят инструментальные средства создания, тестирования и выполнения прикладного программного обеспечения — системы управления базами данных, связующее программное обеспечение, среды исполнения языков программирования — предоставляемые облачным провайдером.

Контроль и управление основной физической и виртуальной инфраструктурой облака, в том числе сети, серверов, операционных систем, хранения осуществляется облачным провайдером, за исключением разработанных или установленных приложений, а также, по возможности, параметров конфигурации среды (платформы).

Инфраструктура как услуга

Инфраструктура как услуга (IaaS, англ. IaaS or Infrastructure-as-a-Service) предоставляется как возможность использования облачной инфраструктуры для самостоятельного управления ресурсами обработки, хранения, сетей и другими фундаментальными вычислительными ресурсами, например, потребитель может устанавливать и запускать произвольное программное обеспечение, которое может включать в себя операционные системы, платформенное и прикладное программное обеспечение. Потребитель может контролировать операционные системы, виртуальные системы хранения данных и установленные приложения, а также ограниченный контроль набора доступных сервисов (например, межсетевой экран, DNS). Контроль и управление основной физической и виртуальной инфраструктурой облака, в том числе сети, серверов, типов используемых операционных систем, систем хранения осуществляется облачным провайдером.

Четыре модели развёртывания [46,23]:

• Частные облака – Предназначены для исключительного использования одной организацией и обычно контролируются, управляются и хостируются частными центрами данных. Хостинг и управление частными облаками могут быть переданы на аутсорсинг внешнему сервис-провайдеру, но частное облако остаётся в исключительном пользовании одной организации.

• Публичные облака – Используются многими организациями (пользователями) совместно, обслуживаются и управляются внешними сервис-провайдерами.

• Групповые облака – Используются группой родственных организаций, желающих воспользоваться общей облачной вычислительной средой. Например, группу могут составить различные рода вооружённых сил, все университеты данного региона или все поставщики крупного производителя.

• Гибридные облака – Появляются, когда организация использует и частное, и публичное облака для одного и того же приложения, чтобы воспользоваться преимуществами обоих. Например, при «ливневом» сценарии организация в случае стандартной нагрузки на приложение пользуется частным облаком, а когда нагрузка пиковая, например, в конце квартала или в праздничный сезон, задействует потенциал публичного облака, впоследствии возвращая эти ресурсы в общий пул, когда они больше не нужны.

Услугами облачных вычислений пользуются в основном компании, которые хотят сэкономить на покупке вычислительных ресурсов, так как за умеренную плату можно получить доступ к самому мощному и современному оборудованию. Так же при выборе SaaS и PaaS возможно отказаться от привлечения дополнительных работников, обслуживающих ИТ-деятельность компании, так как облака обслуживаются продавцом данных услуг.




Компоненты облачных вычислений

Модель облачных вычислений [46] состоит из внешней (front end) и внутренней (back end) частей. Эти два элемента соединены по сети, в большинстве случаев через Интернет. Посредством внешней части пользователь взаимодействует с системой; внутренняя часть – это собственно само облако. Внешняя часть состоит из клиентского компьютера или сети компьютеров предприятия и приложений, используемых для доступа к облаку. Внутренняя часть предоставляет приложения, компьютеры, серверы и хранилища данных, создающие облако сервисов.

Уровни [46]: вычисления как коммунальный ресурс

Концепция облака основана на уровнях, каждый из которых предоставляет определенную функциональность. Такая стратификация компонентов облака позволяет сделать уровни облачных вычислений коммунальным ресурсом, аналогичным электричеству, услугам телефонии или природному газу. Товар "облачные вычисления" - это более дешевые и менее затратные для пользователя вычислительные ресурсы. У облачных вычислений есть все шансы стать еще одним коммунальным ресурсом.

Монитор виртуальных машин [46] (virtual machine monitor - VMM) предоставляет средства для одновременного использования функциональных возможностей облака (см. рисунок 1). VMM – это программа, выполняющаяся на хост-системе и позволяющая одному компьютеру поддерживать несколько идентичных сред исполнения программ. С точки зрения пользователя система представляет собой автономный компьютер, изолированный от других пользователей. В действительности все пользователи обслуживаются одним и тем же компьютером. Виртуальная машина – это одна операционная система (ОС), управляемая основной контролирующей программой, которая представляет ее в виде нескольких операционных систем. При облачных вычислениях VMM предоставляет пользователям возможность отслеживать и, следовательно, управлять такими аспектами процесса, как доступ к данным, хранение данных, шифрование, адресация, топология и перемещение рабочей нагрузки.

Уровни облачной системы

Облако предоставляет следующие уровни [46, 14]:

Уровень инфраструктуры – это основа облака. Он состоит из физических активов – серверов, сетевых устройств, дисков и т.д. Существуют поставщики инфраструктуры как сервиса (Infrastructure as a Service - IaaS). При взаимодействии с IaaS вы в действительности не управляете базовой инфраструктурой, однако управляете операционными системами, хранилищами данных, развертываемыми приложениями и, до определенной степени, выбранными сетевыми компонентами. 

Примером организаций, которые могут получить выгоды от IaaS, являются сервисы печати по требованию (Print On Demand - POD). Модель POD основана на продаже товаров, дизайн которых задается в соответствии с требованиями клиента. POD позволяет физическим лицам открывать магазины и продавать дизайны товаров. Владельцы магазинов могут загрузить столько дизайнов, сколько будут в состоянии создать. Многие загружают тысячи дизайнов. Благодаря возможностям облачной системы хранения POD может предоставлять неограниченный объем дискового пространства.

Промежуточным уровнем является платформа. Она предоставляет инфраструктуру приложений. Платформа как сервис (Platform as a Service - PaaS) предоставляет доступ к операционным системам и соответствующим сервисам. Она дает способ развертывания приложений в облаке при помощи языков программирования и инструментальных средств, поддерживаемых поставщиком. Вам не нужно управлять используемой инфраструктурой или контролировать ее, но у вас есть возможность управлять развернутыми приложениями и, до определенной степени, конфигурациями среды хостинга приложений. 

Существуют поставщики PaaS, например Elastic Compute Cloud (EC2) от Amazon. Идеальный пользователь PaaS – это небольшая частная фирма по созданию программного обеспечения. Имея в своем распоряжении такую платформу, можно создавать продукты мирового класса без накладных расходов, свойственных разработке на собственных ресурсах.

Верхний уровень – это уровень приложений, который обычно и изображают в виде облака. Приложения, выполняющиеся в нем, предоставляются пользователям по требованию. Существуют поставщики программного обеспечения как сервиса (Software as a Service - SaaS), например, Google Pack. Google Pack содержит доступные через Интернет приложения - Calendar, Gmail, Google Talk, Docs и многие другие.



Все эти уровни показаны на рисунке 1 [45]. 





Рис.1. Уровни облачных вычислений, встроенные в компоненты "как сервис". 

Облачные структуры

По характеру владения облачные структуры делятся на три типа: закрытые, открытые и гибридные.[14,46,47]

Открытые облака доступны широкой общественности или большой промышленной группе; они принадлежат и поддерживаются организацией, продающей облачные сервисы. Под "облаком" обычно подразумевается именно открытое облако; посредством Web-приложений сторонний поставщик динамически предоставляет через Интернет ресурсы совместного использования и выставляет счета в зависимости от их использования.

Закрытые облака расположены за сетевым экраном предприятия и управляются этим предприятием. Это облачные сервисы, создаваемые и управляемые внутри предприятия. Закрытые облака предлагают в основном те же преимущества, что и открытые; основное их отличие состоит в том, что ответственность за настройку и поддержку закрытого облака несет предприятие. 

Гибридные облака – это комбинация открытого и закрытого облака, в которой используются сервисы, расположенные как в открытом, так и в закрытом пространстве. Ответственность за управление такими сервисами распределяется между поставщиком открытого облака и предприятием. Используя гибридное облако, организации могут определить цели и требования к создаваемым сервисам и получить их, основываясь на выборе наиболее подходящего варианта.



Выводы

В данном разделе сделан краткий обзор «облачных» технологий, а также моделей, типов и структур «облаков». Также были рассмотрены основные понятия, связанные с облачными технологиями.

По результатам обзора можно сделать вывод, что облачные технологии являются достаточно универсальным средством предоставления вычислительных услуг, обладают возможностью адаптации к запросам пользователей и являются перспективным направлением развития IT систем.




Глава 2. Гипервизоры



Определение

Гипервизор (Hypervisor) — в   или аппаратная схема, обеспечивающая или позволяющая одновременное, параллельное выполнение нескольких или даже многих  на одном и том же хост-компьютере. [47,49,50] Гипервизор также обеспечивает изоляцию операционных систем друг от друга, защиту и безопасность, разделение ресурсов между различными запущенными ОС и управление ресурсами.

Гипервизор сам по себе в некотором роде является минимальной операционной системой ( или ). Он предоставляет запущенным под его управлением операционным системам сервис , виртуализируя или эмулируя реальное (физическое) аппаратное обеспечение конкретной машины, и управляет этими виртуальными машинами, выделением и освобождением ресурсов для них. Гипервизор позволяет независимое «включение», перезагрузку, «выключение» любой из виртуальных машин с той или иной ОС. При этом операционная система, работающая в виртуальной машине под управлением гипервизора, может, но не обязана «знать», что она выполняется в виртуальной машине, а не на реальном аппаратном обеспечении. 



Основные сведения

Облачные технологии строятся на основе виртуализации.

При создании системы облачных вычислений необходимо обеспечить быструю, удобную и бесперебойную работу системы и работающих в ней приложений для каждого конечного потребителя «облака». Для этого требуется оптимизировать нагрузку на облачную систему в целом, а также на каждую клиентскую виртуальную машину. При создании «облака» нужно распределить ресурсы сервера таким образом, чтобы каждый пользователь, который будет работать в нем, мог решать требуемые задачи быстро и качественно. Ресурсы, которые требуют оптимизации, будут следующие: Память, процессор, жесткий диск и сеть. Оптимальное соотношение выделенных ресурсов будет рассчитано в процессе тестирования системы и нагрузки на неё, а также на каждую виртуальную машину в отдельности. Следует заметить, что все ресурсы на гостевых (клиентских) виртуальных машинах - виртуальные. 

Основой для решения данной задачи является гипервизор. Это программа или аппаратная схема, обеспечивающая или позволяющая одновременное, параллельное выполнение нескольких или даже многих операционных систем на одном и том же хост-компьютере. Гипервизор также обеспечивает изоляцию операционных систем друг от друга, защиту и безопасность, разделение ресурсов между различными запущенными ОС и управление ресурсами. [43,50]

Гипервизор управляет n-количеством операционных систем на физическом сервере. 

Данная схема позволяет использовать мощности сервера несколькими пользователями сразу.

Он также отвечает за управление сервером и гостевыми машинами, оптимизацию нагрузки на систему в целом, организацию связи между виртуальными ресурсами и физическими, и многие другие задачи.  Для того, чтобы оптимально организовать работу гипервизора, следует провести тесты системы в различных конфигурациях и ситуациях, и определить как в них будет протекать работа «облака».

Типы гипервизоров

Существует два типа гипервизоров [43,46,50]:  Автономный гипервизор (1 тип, работает непосредственно на оборудовании) и второй тип – На основе базовой ОС (работает поверх установленной основной ОС). Так же существует гибридный тип гипервизоров  (cостоит из двух частей: из тонкого гипервизора, контролирующего процессор и память, а также работающей под его управлением сервисной ОС в кольце пониженного уровня.) Подробнее на рис.2 [46].



Рис.2. Схема типов гипервизора





Принцип работы

Принцип работы у обоих типов гипервизора одинаковый – распределение ресурсов оборудования между клиентами системы, вне зависимости от того на каком уровне работает гипервизор. Гипервизоры первого типа работают быстрее, так как они функционируют непосредственно на оборудовании системы. Так же гипервизоры первого типа имеют больший набор функций для виртуализации системы. Гипервизор первого типа загружается до запуска операционной системы и контролирует все привелигерованные операции, выполняемые операционными системами. Гипервизоры второго типа загружаются после загрузки ядра операционной системы и сосуществуют с ними на равных условиях[9,50].

Изоляция[43]. В данном сценарии клиенты запускают несколько изолированных вычислительных сред на одном устройстве и разделяют данные среды на основании уровней безопасности. Клиентские гипервизоры (второго типа) реализуют данный сценарий посредством запуска — и изоляции — нескольких экземпляров ОС на одном устройстве. Однако инфраструктура виртуальных рабочих столов (VDI) также может реализовывать данный сценарий и предлагает более высокий уровень разделения и защиты данных. VDI хранит данные и приложения в центре обработки данных, а пользователю предоставляет только обзор рабочей станции. Гипервизоры первого типа изначально полностью изолируют операционные системы, функционирующие на сервере, друг от друга.

Управление ресурсами

Гипервизор первого типа полностью берет под контроль все ресурсы оборудования и занимается их распределением между гостевыми ОС. Он имеет встроенные наборы драйверов устройств, компонентов, планировщик заданий, и поэтому не зависит от базовой ОС.

Гипервизор второго типа работает поверх базовой ОС, поэтому гостевой код может выполняться прямо на физическом процессоре, но доступ к устройствам ввода-вывода компьютера из гостевой ОС осуществляется через второй компонент, обычный процесс основной ОС — монитор уровня пользователя.[43, 47, 50]

Продукты гипервизорных решений

В настоящее время на рынке существует множество предложений по гипервизорам от различных производителей.

Самыми распространенными являются [47, 50]:

Первый тип – Hyper-V, XEN Server, VMware ESX Server, KVM.

Второй тип – VMware Workstation, Microsoft VirtualPC, VirtualBOX, QUEMU

Гибридный тип – Microsoft Client Hyper-V, Citrix

Ниже приведены схемы сравнения некоторых гипервизорных решений [47].





	   Рис.3.1. Схема сравнения аналогов гипервизора. 




	   Рис.3.2. Схема сравнения аналогов гипервизора.



	Рис.3.3. Схема сравнения гипервизоров.



Выводы



В данной главе были приведены основные сведения о гипервизорах – их типах,  возможностях, а также показаны самые  актуальные на данный момент гипервизорные решения. 

Показано, что гипервизор явяляетсяя основным инструментом управления работой облачной системы.




Постановка задачи



Для выполнения дипломного проекта требуется:

Создать стенд облачной системы.

Организовать работу гипервизора, обеспечивающего платформу для функционирования облачной системы, а также управление ресурсами системы.

Провести эксперименты (тестирование) для отладки работы системы и непосредственно организации работы гипервизора.

Разработать математическую модель для оценки эффективности работы гипервизора при планировании процессорного времени, выделяемого виртуальным машинам.

Провести анализ результатов, полученных в результате экспериментов.

Сделать выводы по результатам анализа




Глава 3. Организация работы гипервизора при создании систем облачных вычислений



Задачи создания стенда облачной системы

В данном проекте будет создан экспериментальный стенд облачной системы, работающей по принципу закрытого частного облака с моделью развертывания PaaS [12,14,29,41] – платформа как услуга. Далее будут проведены эксперименты (тестирование) в облачной системе и анализ результатов, после чего будут сделаны выводы.

Для реализации данной задачи будет использоваться гипервизор гибридного типа Microsoft Client Hyper-V [5,18, 24, 44 ], встроенный в новый Windows 8 Professional x64. Данный гипервизор является наследником технологий виртуализации, использующихся в гипервизоре Hyper-V Server. 

На стенде облачной системы, созданном с помощью Client Hyper-V, каждая ВМ имеет свой раздел, в пределах которого она работает, т.е. виртуальные машины не зависимы друг от друга. 

Различия клиентской и серверной версии Hyper-V[51]:

Отсутствие репликации 

Отсутствие возможности виртуализировать сеть

Отсутствие возможности живой миграции виртуальных машин

Отсутствие поддержки SR-IOV ( Из-за наличия только на серверном оборудовании)

Отсутствие виртуального Fiber Channel

Отсутствие поддержки технологии RemoteFX GPU Hardware Acceleration)

В остальном все функциональные возможности совпадают. [13,24,51]

При выборе гипервизора для создания облачной системы, я руководствовался такими принципами, как бесплатность, доступность, функциональность, удобство работы, надежность, безопасность, а также производительность. 


 Описание стенда

Стенд представляет собой сервер, на котором функционируют две виртуальные машины и гипервизор Microsoft Client Hyper-V.

Конфигурация оборудования:

Сервер:

Процессор:Intel Core i3 M370 2.40GHZ 

Оперативная память: 3 GB RAM

Жесткий диск: Seagate Momentus 5400.6 ST9250315AS 250 GB HDD 5400 об/мин.

Операционная система: Windows 8 Professonal x64 bit

Сетевой адаптер: Qualcomm Atheros AR8152 PCI-E Fast Ethernet Controller (NDIS 6.30)

Скорость передачи данных по сети 54 Mbit/s.

Гипервизор:

Microsoft Client Hyper-V

Виртуальные машины (клиенты облачной системы):

Ресурсы, предоставляемые гипервизором виртуальным машинам – виртуальные, а всё оборудование – эмулированное.

Для всех виртуальных машин, функционирующих на сервере, конфигурация оборудования одинаковая:

Процессор: Виртуальный процессор, соответствующий по характеристикам Intel Core i3 M370 2.40GHZ, но с условием, что каждая ВМ может потреблять только 30% мощности сервера

Память: 512 MB RAM – динамическая виртуальная память с возможностью расширения в режиме реального времени до 1000 MB без участия пользователя

Жесткий диск: Виртуальный динамический жесткий диск объемом 20 GB

Операционная система: Windows 7 Ultimate x64 bit

Сеть: Сеть построена с использованием виртуального коммутатора. 


Описание работы Microsoft Client Hyper-V

Общее описание

Гипервизор Client Hyper-V [44, 51],  с помощью которого будет создан прототип облачной системы, является гибридным гипервизором. Хотя он входит в состав хостовой ОС (Windows 8), и по определению должен относится к гипервизорам второго типа, это не совсем так. По умолчанию, компоненты Hyper-V отключены в системе и гипервизор не активен. Все управление компьютером лежит на операционной системе. При включении компонентов данного гипервизора, происходит установка управляющей роли Hyper-V. После этого Windows 8 становится виртуализирована и назначается хостовой(родительской) ОС. В этом случае хостовая ОС запускается в таком же виртуальном окружении, как и все виртуальные машины, и именуется «родительским разделом». Все остальные окружения, соответственно – «дочерние». Единственная разница между родительским и дочерними разделами состоит в том, что только родительский раздел имеет непосредственный доступ к оборудованию сервера. Выделением памяти же и планировкой процессорного времени занимается сам гипервизор.

Схема работы гибридного гипервизора: 



Рис.4. Схема работы гибридного гипервизора






Схема работы гипервизора Client hyper-V [24]:



Рис.5. Схема работы гипервизора Client Hyper-V.





Рис.5.1. Схема работы гипервизора Microsoft Client Hyper-V [18].






Гипервизор распределяет ресурсы на основе заданных параметров для памяти, процессора, дисков, сети и т.д., а также правил распределения этих ресурсов.  Но важно заметить, что все ресурсы заданы как динамически расширяемые, т.е. если конечному пользователю для решения задач не достаточно имеющихся вычислительных ресурсов, гипервизор автоматически добавит из имеющегося резерва дополнительные мощности. Но и здесь существуют заданные ограничения по расширению ресурсов, а также ограниченные возможности физического оборудования. [18,24,47]

Гипервизор управляет ресурсами так, чтобы не случалось перегрузок системы. ВМ с наибольшим весом ресурсов получает их в первую очередь, и выгрузка ресурсов происходит из данной ВМ в последнюю очередь. В случае, если ресурсы требуются обеим ВМ, первой получает машина с наибольшим весом конкретного ресурса. Например, для памяти: До тех пор, пока у сервера имеется свободная память – веса виртуальных машин не играют никакой роли. Но как только память подходит к концу и нужно ее перераспределить – гипервизор отбирает память у виртуальных машин с наименьшим приоритетом, и отдает тем, у кого приоритет наиболее высокий.[30]

Архитектура Client Hyper-V

Архитектура Client Hyper-V не отличается от серверной Hyper-V.

Основные термины архитектуры Hyper-V [18,24]:

Hyper-V – технология виртуализации на основе гипервизора для x64 систем.

Раздел (partition) - логическая единица разграничения, поддерживаемая гипервизором, в которой функционируют операционные системы.

VID (Virtualization Infrastructure Driver) - драйвер виртуальной машины. Осуществляет управление разделами, процессами виртуальной машины и памятью. Является промежуточным звеном между гипервизором и стеком виртуализации.

VM шина - обеспечивает высокоскоростное взаимодействие между родительским и дочерними разделами.

VSC (Virtualization Service Client) - клиент служб виртуализации. Располагается в дочерних разделах, перенаправляет запросы гостевой операционной системы родительскому разделу, через VM шину.

VSP (Virtualization Service Provider) - провайдер служб виртуализации. Располагаются в родительском разделе и обеспечивают доступ дочерних разделов к аппаратным ресурсам.

WinHv (Windows Hypervisor Library) - мост между драйверами операционных систем и гипервизором, который позволяет драйверам вызывать гипервизор с использованием стандартного для Windows соглашения о вызовах.

Принцип устройства и работы Microsoft Client Hyper-V 



Родительский раздел

Родительский раздел [3,5,18,24] назначается сразу же при установке системной роли Hyper-V. Компоненты родительского раздела показаны на рис. ниже. 

Назначение родительского раздела следующее [5,18,24]:

Создание, удаление и управление дочерними разделами, в том числе и удаленное, посредством WMI-провайдера.

Управление доступом к аппаратным устройствам, за исключением выделения процессорного времени и памяти – этим занимается гипервизор.

Управление питанием и обработка аппаратных ошибок, если таковые возникают.



Рис.6. Устройство родительского раздела Microsoft Client Hyper-V [24]



Следующие компоненты, работающие в родительском разделе, в совокупности называют стеком виртуализации [24]:

Служба управления виртуальными машинами (VMMS)

Рабочие процессы виртуальных машин (VMWP)

Виртуальные устройства

Драйвер виртуальной инфраструктуры (VID)

Библиотека интерфейсов гипервизора



Помимо этого, в родительском разделе работают еще два компонента. Это провайдеры служб виртуализации (VSP) [5,18,24] и шина виртуальных машин (VMBus).



Служба управления виртуальными машинами [18,24]

В задачи службы управления виртуальными машинами (VMMS) входит:

Управление состоянием виртуальных машин (включено/выключено)

Добавление/удаление виртуальных устройств

Управление моментальными снимками



При запуске виртуальной машины VMMS создает новый рабочий процесс виртуальной машины. Так же именно VMMS определяет, какие операции разрешено выполнять с виртуальной машиной в настоящий момент: к примеру, если происходит удаление снимка, то применить снимок в течение операции удаления она не даст. Если говорить более детально – то VMMS  [18,24] управляет следующими состояниями виртуальных машин:

Starting

Active

Not Active

Taking Snapshot

Applying Snapshot

Deleting Snapshot

Merging Disk



Другие задачи управления – Pause, Save и Power Off – выполняются не службой VMMS, а непосредственно рабочим процессом соответствующей виртуальной машины. Служба VMMS работает как на уровне пользователя, так и на уровне ядра как системная служба (VMMS.exe) и зависит от служб Remote Procedure Call (RPC) и Windows Management Instrumentation (WMI). Служба VMMS включает в себя множество компонент, среди которых имеется и WMI-провайдер, предоставляющий интерфейс для управления виртуальными машинами. Благодаря этому можно управлять виртуальными машинами из командной строки и с помощью скриптов VBScript и PowerShell. System Center Virtual Machine Manager так же использует этот интерфейс для управления виртуальными машинами.

Рабочий процесс виртуальной машины (VMWP)[18,24]

Для управления виртуальной машиной из родительского раздела запускается особый процесс – рабочий процесс виртуальной машины (VMWP). Процесс этот работает на уровне пользователя. Для каждой запущенной виртуальной машины служба VMMS запускает отдельный рабочий процесс. Это позволяет изолировать виртуальные машины друг от друга. Для повышения безопасности, рабочие процессы запускаются под встроенным пользовательским аккаунтом Network Service. Процесс VMWP используется для управления соответствующей виртуальной машиной. В его задачи входит: Создание, конфигурация и запуск виртуальной машины, Пауза и продолжение работы (Pause/Resume) Сохранение и восстановление состояния (Save/Restore State),Создание моментальных снимков (снапшотов),Кроме того, именно рабочий процесс эмулирует виртуальную материнскую плату (VMB), которая используется для предоставления памяти гостевой ОС, управления прерываниями и виртуальными устройствами.,



Виртуальные устройства [18,24]

Виртуальные устройства (VDevs) – это программные модули, реализующие конфигурацию и управление устройствами для виртуальных машин. VMB включает в себя базовый набор виртуальных устройств, включающий в себя шину PCI и системные устройства. Есть два типа виртуальных устройств:

Эмулируемые устройства – эмулируют определенные аппаратные устройства, такие, к примеру, как видеоадаптер VESA. Эмулируемых устройств достаточно много, к примеру: BIOS, DMA, APIC, шины ISA и PCI, контроллеры прерываний, таймеры, управление питанием, контроллеры последовательных портов, системный динамик, контроллер PS/2 клавиатуры и мыши, эмулируемый (Legacy) Ethernet-адаптер (DEC/Intel 21140), FDD, IDE-контроллер и видеоадаптер VESA/VGA. Именно поэтому для загрузки гостевой ОС может использоваться только виртуальный IDE-контроллер, а не SCSI, который является синтетическим устройством.

Синтетические устройства – не эмулируют реально существующие в природе железки. Примерами служат синтетический видеоадаптер, устройства взаимодействия с человеком (HID), сетевой адаптер, SCSI-контроллер, синтетический контроллер прерывания и контроллер памяти. Синтетические устройства могут использоваться только при условии установки компонент интеграции в гостевой ОС. Синтетические устройства обращаются к аппаратным устройствам сервера посредством провайдеров служб виртуализации, работающих в родительском разделе. Обращение идет через виртуальную шину VMBus, что намного быстрее, чем эмуляция физических устройств.



Драйвер виртуальной инфраструктуры (VID) [18,24]

Драйвер виртуальной инфраструктуры (vid.sys) работает на уровне ядра и осуществляет управление разделами, виртуальными процессорами и памятью. Так же этот драйвер является промежуточным звеном между гипервизором и компонентами стека виртуализации уровня пользователя.

Библиотека интерфейса гипервизора [18,24]

Библиотека интерфейса гипервизора (WinHv.sys) – это DLL уровня ядра, которая загружается как в хостовой, так и в гостевых ОС, при условии установки компонент интеграции. Эта библиотека предоставляет интерфейс гипервызовов, использующийся для взаимодействия ОС и гипервизора.

Провайдеры служб виртуализации (VSP) [18,24]

Провайдеры служб виртуализации работают в родительском разделе и предоставляют гостевым ОС доступ к аппаратным устройствам через клиент служб виртуализации (VSC). Связь между VSP и VSC осуществляется через виртуальную шину VMBus.

Шина виртуальных машин (VMBus) [18,24]

Назначение VMBus состоит в предоставлении высокоскоростного доступа между родительским и дочерним разделами, в то время как остальные способы доступа значительно медленнее из-за высоких накладных расходах при эмуляции устройств. Если гостевая ОС не поддерживает работу интеграционных компонент – приходится использовать эмуляцию устройств. Это означает, что гипервизору приходится перехватывать вызовы гостевых ОС и перенаправлять их к эмулируемым устройствам, которые, напоминаю, эмулируются рабочим процессом виртуальной машины. Поскольку рабочий процесс запускается в пространстве пользователя, использование эмулируемых устройств приводит к значительному снижению производительности по сравнению с использованием VMBus. Именно поэтому рекомендуется устанавливать компоненты интеграции сразу же после установки гостевой ОС. Как уже было сказано, при использовании VMBus взаимодействие между хостовой и гостевой ОС происходит по клиент-серверной модели. В родительском разделе запущены провайдеры служб виртуализации (VSP), которые являются серверной частью, а в дочернем разделе – клиентская часть – VSC. VSC перенаправляет запросы гостевой ОС через VMBus к VSP в родительском разделе, а сам VSP переадресовывает запрос драйверу устройства. Этот процесс взаимодействия абсолютно прозрачен для гостевой ОС. 




Дочерние разделы

Вернемся к нашему рисунку с архитектурой Hyper-V [4,5,18,24], только немного сократим его, поскольку нас интересуют лишь дочерние разделы. 

Рис. 8 Дочерние разделы [24]



Итак, в дочерних разделах могут быть установлены:

ОС Windows, с установленными компонентами интеграции (в нашем случае – Windows 7)

ОС не из семейства Windows, но поддерживающая компоненты интеграции (Red Hat Enterprise Linux в нашем случае)

ОС, не поддерживающие компоненты интеграции (например, FreeBSD).


Во всех трех случаях набор компонент в дочерних разделах будет немного различаться.

Остановимся на рассмотрении ОС Windows с установленными компонентами интеграции. Операционные системы Microsoft Windows [5,6], начиная с Windows 2000 поддерживают установку компонент интеграции. После установки Hyper-V Integration Services [5,6,24] в гостевой ОС запускаются следуюшие компоненты: 

Клиенты служб виртуализации. VSC представляют собой синтетические устройства, позволяющие осуществлять доступ к физическим устройствам посредством VMBus через VSP. VSC появляются в системе только после установки компонент интеграции, и позволяют использовать синтетические устройства. Без установки интеграционных компонент гостевая ОС может использовать только эмулируемые устройства. ОС Windows 7 и Windows Server 2008 R2 включает в себя компоненты интеграции, так что их не нужно устанавливать дополнительно.

Улучшения. Под этим имеются в виду модификации в коде ОС чтобы обеспечить работу ОС с гипервизором и тем самым повысить эффективность ее работы в виртуальной среде. Эти модификации касаются дисковой, сетевой, графической подсистем и подсистемы ввода-вывода. Windows Server 2008 R2 и Windows 7 уже содержат в себе необходимые модификации, на другие поддерживаемые ОС для этого необходимо установить компоненты интеграции.



Так же, компоненты интеграции предоставляют следующий функционал: 

Heartbeat (пульс) – помогает определить, отвечает ли дочерний раздел на запросы из родительского.

Обмен ключами реестра – позволяет обмениваться ключами реестра между дочерним и родительским разделом.

Синхронизация времени между хостовой и гостевой ОС

Завершение работы гостевой ОС

Служба теневого копирования томов (VSS), позволяющая получать консистентные резервные копии.

Описание особенностей и правил управления ресурсами в Microsoft Client Hyper-V

Память

Память ВМ является динамической [19,21,27,30,31]. Механизм работы динамической памяти следующий. В гостевой ОС (при установке служб интеграции) работает компонент Dynamic Memory VSC (DMVSC), который собирает сведения об используемой в данный момент памяти и передает их посредством виртуальной шины VMBus провайдеру Dynamic Memory VSP (DMVSP), работающему в хостовой ОС. DMVSP, в свою очередь, передает данные балансировщику памяти – Dynamic Memory Balancer [30,31].

Балансировщик вычисляет для каждой виртуальной машины некий идеальный объем памяти. Формула достаточно проста: идеальный объем равен потребляемой в текущий момент памяти плюс резерв (определенный процент от потребляемой памяти, задаваемый вручную) [30].

Затем вычисляется значение нагрузки Memory Pressure – процентное отношение вычисленного идеального объема к объему, фактически выделенному виртуальной машине. Этот параметр показывает, как на данный момент обстоят дела с памятью на виртуальной машине: малые значения означают, что памяти достаточно, большие – что использование памяти подходит к пределу. Если же значение Memory Pressure превышает 100%, то значит памяти сильно не хватает, и виртуальная машина активно использует файл подкачки [30].

На основании изменений нагрузки в течение времени балансировщик вычисляет пороговые значения нагрузки – минимальное и максимальное. Если нагрузка превышает максимальное пороговое значение – то виртуальной машине добавляется память. Если же значение нагрузки опускается ниже минимального порогового значения – это означает, что излишек памяти можно отобрать [30].

Добавление памяти. 

Диспетчер памяти стека виртуализации [5,27,30]выделяет виртуальной машине дополнительную память. DMVSC, используя технологию горячего добавления памяти (Hot Add RAM) расширяет адресное пространство виртуальной машины, после чего соответствующие виртуальные адреса сопоставляются с выделенными физическими адресами. Обратите внимание, что для работы необходима поддержка Hot Add RAM на уровне гостевой ОС [30].

Удаление памяти. 

Для удаления памяти используется механизм Memory Ballooning. Когда приходит команда на удаление памяти – DMVSC проверяет, какие области памяти на данный момент не используются. Из них отбирается объем, предназначенный для удаления, и затем эти адреса захватываются DMVSC в монопольный доступ. После этого захвата область памяти помечается как Driver Locked и становится недоступна для использования операционной системой. Как только память была захвачена — соответствующие виртуальные адреса отвязываются от физических адресов, и соответствующие ячейки памяти могут быть переданы другим виртуальным машинам.  Удаление памяти происходит для системы абсолютно незаметно, вся захваченная память по-прежнему остается системе видна. В дальнейшем, если виртуальной машине нужно добавить память – соответствующее адресное пространство освобождается и связывается с выделенной областью памяти [30].

Smart Paging

В случае недостатки памяти для запуска ВМ, в Hyper-V предусмотрена технология Smart Paging, позволяющая использовать файл подкачки на хосте (сервере) – Smart Paging file. 

Для работы виртуальные машины имеют свой собственный файл подкачки, который и используют в случае нехватки памяти. Данный подход более эффективен, чем файл подкачки на хосте, так как диспетчер памяти внутри виртуальной машины лучше знает, какие процессы можно поместить в файл подкачки, а какие должны находится в оперативной памяти. Smart Paging используется исключительно для перезагрузки виртуальных машин [30].

Жесткий диск

Виртуальная машина на Hyper-V может использовать три вида виртуальных жестких дисков [52]. Все три вида представляют собой обычный файл, расположенный в разделе NTFS [52].

Рассмотрим подробнее каждый вид:

Dynamically expanding disk - динамически расширяющийся диск. Этот диск используется по умолчанию при создании виртуальной машины. Виртуальная машина, использующая этот диск, видит полный размер диска. Однако, в файловой системе хоста, файл жесткого диска занимает столько места, сколько занимают данные в виртуальной машине. По мере роста объема данных в виртуальной машине, растет и размер файла жесткого диска. Этот вид диска очень удобно использовать в тестовых средах, поскольку производительность виртуальных машин с данным диском меньше, чем виртуальных машин с дисками фиксированного объема [52].

Fixed size disk - диски фиксированного размера. Для дисков данного вида характерно то, что на хостовой машине создается файл равный размеру жесткого диска виртуальной машины. Например, если вы создаете в виртуальной машине жесткий диск 40 Гб, то файл на хосте будет также размером 40 Гб. После создания файла свободное место в нем заполняется нулями. Диски фиксированного размера должны по умолчанию использоваться в рабочей среде.[52]

Differencing disk - разностный жесткий диск. Диски данного вида обладают взаимосвязью родительский-дочерний. Родительский диск это статический диск, предназначенный только для чтения. Разностный диск (дочерний) сохраняет все изменения. Используя этот вид диска, можно создать несколько виртуальных машин с одним родительским жестким диском. При этом разностный диск будет у каждой машины свой [52].

Виртуальная машина может использовать любой вид виртуальных жестких дисков. Однако существует возможность использовать и физические жесткие диски. Для этого используется pass-through (сквозное) подключение жесткого диска [52].

Чтобы использовать такое подключение, жесткие диски должны быть видны на хоствой машине. Это могут быть локальные диски хоста, диски iSCSI или SAN. Нельзя подключить только определенный раздел жесткого диска, жесткий диск должен быть подключен целиком. Для использования pass-through подключения, жесткий диск на локальной машине должен находиться в Offline. Переключать жесткий диск в online или offline можно, используя Disk Manager или утилиту diskpart [52].

В теории pass-through подключение должно обеспечивать максимальную производительность. Но, по результатам тестов, виртуальный Fixed disk совсем немного уступает сквозному подключению жестких дисков.

В случае использования физических дисков, напрямую подключенных к виртуальной машине, необходимо учитывать следующее [52]:

Данный тип дисков не поддерживает динамическое расширение.

С ними нельзя использовать разностные диски.

Нельзя создавать снимки виртуальных жестких дисков.

Ограничения виртуальных жестких дисков:

Файлы виртуальных жестких дисков должны располагаться на NTFS разделе;

Максимальный размер файла виртуального жесткого диска не должен превышать 2040 GB (2 TB);

Нельзя использовать сжатие папок, где расположены файлы VHD.

Виртуальные сети в Microsoft Client Hyper-V

В Hyper-V гостевые ОС никогда не имеют прямого доступа к аппаратному оборудованию, а интерфейсы управления Hyper-V контролируют трафик, проходящий через физические и виртуальные интерфейсы. Hyper-V имеет диспетчер виртуальных сетей (Virtual Network Manager). Диспетчер виртуальных сетей отвечает за создание и управлением виртуальными коммутаторами. Количество создаваемых виртуальных коммутаторов не ограничено,‘ оно зависит от типа виртуальной сети, с которой проходит работа. Например, внешние виртуальные сети обычно расположены на физических сетевых картах (NIC), поэтому количество внешних сетей будет равно количеству физических сетевых карт [25,51].

Заметным отличием клиентского Hyper-V от серверного является его способность работы с беспроводными адаптерами. Сетевая инфраструктура Hyper-V основывается на виртуальном коммутаторе, который дифференцирует пакеты по MAC-адресам виртуальных адаптеров. Но по Wi-Fi-каналу передавать пакеты с различными MAC-адресами нельзя, поэтому стандартную схему пришлось несколько скорректировать. Конкретнее, в нее добавили еще одного посредника в виде сетевого моста. Мост просто сопоставляет IP-адрес виртуальной сетевой карты с ее MAC-адресом, что и обеспечивает корректную маршрутизацию пакетов, которые поступают из внешней сети. Естественно, создается и настраивается мост автоматически, как только пользователь выберет соответствующие настройки [25,51].


Процессор

В виртуальных машинах используется виртуальный процессор [32], аналогичных по характеристикам тому, что установлен на сервере, но процессор доступный виртуальной машине получается совсем не такой, каким он является аппаратно. Более того, возможности процессора доступного хост системе с ролью Hyper-V также существенно отличается от оригинальных спецификаций. Идентификатор процессора, формально его имя и номер, пробрасываются в виртуальную машину без изменений. Но часть инструкций ей недоступна [26,32,38,40].

Для виртуальных машин Hyper-V представляет процессоры многоядерными. Для существующего количества виртуальных процессоров, количество ядер на виртуальный процессор равно количеству физических ядер на реальных процессорах. Включение режима совместимости со старыми ОС для многопроцессорной виртуальной машины представляет виртуальные процессоры не как ядра одного физического процессора, а как логические потоки (Hyper-Threading) одноядерного процессора[26,32,38,40].

Hyper-V поддерживает до восьми логических (виртуальных) процессоров на один физический, однако больше четырех логических процессоров задавать для одной виртуальной машины не рекомендуется [26,32,38,40].

Технология NUMA

NUMA (Non-Uniform Memory Access) — «Неравномерный доступ к памяти» или Non-Uniform Memory Architecture — «Архитектура с неравномерной памятью». NUMA система разделяется на множественные узлы, имеющие доступ как к своей локальной памяти, так и к памяти других узлов (называемой «удаленной»). Доступ к удаленной памяти оказывается гораздо медленнее, чем к локальной. Оттуда и название – «неоднородный доступ к памяти». 

Системы NUMA [20,40] состоят из однородных базовых узлов, содержащих небольшое число процессоров с модулями основной памяти.

Hyper-V не использует жесткую привязку к физическому процессору (Processor affinity). Виртуальная машина использует те логические процессоры, которые в настоящее время свободны и предоставляются гипервизору постановщиком ресурсов (Scheduler). Но при этом каждая виртуальная машина имеет специальную характеристику для задания «узла NUMA». Она позволяет гипервизору указать, с каких процессоров и с какой шины памяти будут использоваться ресурсы для данной ВМ [20,40].

Важно заметить, что технологию NUMA нельзя использовать одновременно с технологией динамической памяти.

Каждая виртуальная машина может резервировать для себя ресурсы процессора. Данные ресурсы будут недоступны другим клиентам системы. Гипервизор закрепляет за конкретной виртуальной машиной заданный процент от мощностей сервера. Данная функция очень полезна, так как в процессе функционирования системы, гипервизор постоянно перераспределяет ресурсы сервера между виртуальными машинами, и может возникнуть ситуация при которой требуемых клиентом системы ресурсов может не оказаться в наличии, так как они заняты другими клиентами. При задании резерва ресурсов процессора, в случае высокой нагрузки на систему, виртуальная машина гарантированно получит величину резерва[20,40].

Также, при создании виртуальных машин, существует возможность ограничить доступный для них предел использования ресурсов физического процессора. Данный предел задается в процентном значении от общего количества ресурсов оборудования. Данный параметр позволяет не допустить ситуации, в которой виртуальные машины используют все имеющиеся мощности физического оборудования, в результате чего быстродействие системы уменьшится [20,40].

Данный лимит применяется отдельно к каждой виртуальной машине. 

Вес ресурсов процессора регулирует очередность получения ресурсов при высокой нагрузке на систему. До тех пор, пока в системе имеются в наличии доступные процессорные ресурсы, вес не имеет никакого значения. Но как только виртуальные машины начинают запрашивать больше ресурсов, чем доступно для распределения, вес процессорных ресурсов влияет на очередность и количество ресурсов, которые получит конкретная виртуальная машина. Если у всех клиентов системы вес процессорных ресурсов одинаковый, то каждая ВМ получит равное количество процессорных ресурсов в порядке общей очереди. Если у конкретной ВМ вес ресурсов выше, чем у остальных, данная ВМ получит их быстрее и в большем количестве [20,40].

Процессорное время

Если на физическом оборудовании имеется несколько процессоров, то для каждой виртуальной машины можно назначить конкретный физический процессор. Данная функция позволяет увеличить быстродействие системы в несколько раз за счет уменьшения длины очереди на обработку запросов к процессору от клиентов системы, а также увеличения количества ресурсов оборудования. В данном случае операции обработки данных и вычисления на различных виртуальных машинах  могут производиться параллельно на нескольких физических процессорах. Поддержка многопроцессорных систем важна, если на сервере функционирует большое количество ВМ [15,20,26,32].

В ситуации, когда на сервере имеется всего один физический процессор, виртуальные машины, а также операционная система сервера встают в очередь на получение квантов процессорного времени. Операции вычисления и обработки запросов происходят на физическом процессоре последовательно. Процессорное время распределяется гипервизором на основе заданных приоритетов. Как только ВМ требуется выполнить операции с помощью процессора, она сообщает посредством гипервызовов о своей готовности[15,20,26,32,34].

На рис.9. приведена схема распределения процессорного времени в Microsoft Client Hyper-V.








Рис.9. Распределение процессорного времени между ВМ.



Проведение экспериментов (тестирования системы) на стенде облачной системы

Описание экспериментов 

Существует сервер, а также две виртуальные машины с одинаковыми параметрами, конфигурации которых были указаны в предыдущем разделе. Они соединены между собой в локальную сеть, а также имеется доступ в интернет. Ко всем файлам на виртуальных машинах и некоторым на сервере открыт общий доступ, а так же на сервере создан сетевой диск, доступ к которому может получить  любой клиент системы. На каждой ВМ (Виртуальной машине) установлен некоторый набор приложений, таких как MS Office, средства работы с виртуальными дисковыми накопителями, средства для просмотра аудио/видео файлов, и базовые программы, входящие в состав ОС. 

Сначала мы протестируем, как облачная система будет работать в условиях различной нагрузки на неё.

Инструменты для тестирования

Для тестирования нагрузки будут использоваться:

Для тестирования нагрузки на систему используются следующие инструменты: Монитор ресурсов Windows [37,39], системный монитор Windows, диспетчер задач Windows, Диспетчер Hyper-V,  продукция компании Veeam ONE, Windows power shell [36, 45], планировщик задач ОС.

На сервере – Veeam ONE Monitor для слежения за процессором, памятью и дисками в режиме реального времени. Так же с помощью встроенной в ОС программы «Системный монитор» мы будем записывать логи производительности сервера для последующего анализа нагрузки на оборудование. 

Диспетчер Hyper-V и монитор ресурсов будут вспомогательными приложениями, с помощью которых мы будем также наблюдать за состоянием облачной системы, и сравнивать результаты с полученными из других программ.

На виртуальных машинах также будет использован системный монитор для записи логов производительности каждой машины в отдельности.

Также будет произведен математический расчет по модели гипервизора Microsoft Client Hyper-V.

Для полной картины нагрузки будут использоваться команды модуля Hyper-V [36, 37] в Windows PowerShell [36,45].


Этапы тестирования

Создание тестов производительности

Выполнение тестов

Анализ результатов тестов производительности

Проведение расчетов по математической модели

Оптимизация параметров гипервизора с учетом результатов экспериментов

Создание тестов производительности

Для тестов используется созданный файл с расширением .bat,  при исполнении которого, происходит запуск заданного числа программ с интервалом во времени. На каждой из ВМ создан свой файл с различными инструкциями запуска приложений. Файлы тестирования .bat на двух ВМ различаются интервалом запуска процессов и временем жизни процессов. Оба файла запускаются одновременно с помощью планировщика задач Windows. Вместе с началом выполнения сценариев, запрограммированных в файлах тестирования, начинается сбор сведений о производительности и нагрузке на системы с помощью сборщиков данных системного монитора Windows. Сбор сведений для анализа производится как на ВМ, так и на сервере. Вместе с этим, программа Veeam ONE Monitor, запущенная на сервере, анализирует сведения о нагрузке на сам сервер, а также на гостевые ВМ (т. е. на облачную систему в комплексе). Данное приложение с помощью графиков показывает изменения, происходящие в «облаке» в режиме реального времени. 

Для полноты тестирования, оно проводится несколько раз с различными параметрами и величиной нагрузки.

Для того, чтобы выяснить, как будет реагировать система в целом, а также клиентские машины в отдельности, моделируется нагрузка в различных ситуациях. В файлах .bat для каждой ситуации задается различное время жизни запущенных приложений (процессов) и различный интервал выполнения приложений. 

Тесты:

На 1 ВМ запускаются 11 приложений с интервалом в 1 секунду, на 2 ВМ – 12 приложений. Срок жизни процесса – бессрочно в обоих случаях.

На 1 ВМ запускаются 5 приложений с интервалом 5 секунд, срок жизни – 1 минута у каждого. На 2 ВМ – 10 приложений с интервалом 5 секунд, срок жизни – бессрочно.

На 1 ВМ запускаются 5 приложений с интервалом 30 секунд, срок жизни каждого приложения – 1 минута, на 2 ВМ – 10 приложений с интервалом 30 секунд, срок жизни – бессрочно.

На 1 ВМ запускается 9 приложений с интервалом 1 минута, срок жизни каждого процесса 1 минута, на 2 ВМ – 10 приложений с интервалом в 1 минуту, срок жизни – бессрочно.

На 1 ВМ запускается 10 приложений с интервалом в 5 минут, срок жизни каждого приложения – 1 минута, на 2 ВМ – 10 приложений с интервалом 100 секунд, срок жизни – бессрочно.

В каждом случае запуск файлов .bat начинается одновременно с помощью планировщика задач, чтобы проанализировать, как система реагирует на одновременную загрузку клиентских машин, и как ресурсы будут распределяться между ними в процессе работы. Цель данных тестов – анализ механизма распределения имеющихся физических ресурсов между виртуальными машинами в зависимости от ситуации. Например, как будет себя вести система в случае, если несколько клиентов начнут одновременное выполнение ресурсоемких задач, и как ресурсы будут распределяться в зависимости от освобождения или нехватки в определенный момент времени. Это нужно для оптимизации имеющихся ресурсов с целью повышения производительности системы. 

Пример содержания файла .bat для случая когда приложения открываются с интервалом в 1 секунду:

@Echo Off

start "" "C:\Program Files (x86)\Microsoft Office\Office12\excel.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Microsoft Office\Office12\winword.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Microsoft Office\Office12\msaccess.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Microsoft Office\Office12\powerpnt.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Microsoft Office\Office12\outlook.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Mozilla Firefox\firefox.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Internet Explorer\iexplore.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Windows Media Player\wmplayer.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files (x86)\Alcohol Soft\Alcohol 120\alcohol.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files\Microsoft Games\Solitaire\solitaire.exe"

ping -n 1 localhost>nul

start "" "C:\Program Files\Microsoft Games\MineSweeper\MineSweeper.exe"

exit

 Для изменения интервалов запуска процессов используется команда ping, которая задает задержку во времени между выполнением команды запуска приложений.

Выполнение тестов производительности

В результате тестов было выявлено, что быстродействие системы выше, когда приложения запускаются с наибольшим интервалом во времени, а также начиная с интервалом в больше минуты. Данные результаты говорят о том, что ресурсы, а особенно память, возвращаются гипервизору не сразу после завершения работы процесса, а после некоторого промежутка времени, в течение которого виртуальная машина удерживает их у себя в распоряжении для дальнейшего использования. Данный момент обусловлен тем, что виртуальная машина рассчитывает, что ресурсы могут понадобиться в небольшой промежуток времени после выполнения задачи, и, чтобы не произошло ситуации, когда ресурсы отданы обратно гипервизору для дальнейшего распределения, в то время как виртуальной машине они снова понадобились. 

В процессе экспериментов было замечено, что больше всего высокой нагрузке подвергаются память и жесткий диск. Данная проблема возникает потому, что память является самым часто используемым ресурсом, плюс многие приложения резервируют для себя память, не выгружая её из своего распоряжения. В случае, когда в системе заканчивается доступная память, виртуальные машины начинают использовать файл подкачки, расположенный на жестком диске, в результате чего происходит превышение нагрузки на виртуальные машины и систему.

В случае, когда на обе виртуальные машины была высокая нагрузка, производительность как системы, так и виртуальных машин существенно снижалась, однако это не привело к остановке работы системы.(Рис.10 и Рис.11).

Рис.10. Высокая нагрузка на систему. Win7_2



Рис.10. Высокая нагрузка на систему. Win 7.



Как видно из Рис.10 и Рис.11, при высокой интенсивности запросов пользователей системы, на виртуальных машинах перегрузке подвергаются ОЗУ и диск.  Как было сказано выше, несмотря на эти факторы, системы остается в работоспособном состоянии.

Далее, в соответствии с разработанными тестами, следуют результаты работы системы для каждого из тестов. Поведение системы и виртуальных машин в отдельности можно наблюдать на рис. 11 – рис. 31. Изображения с результатами разбиты по категориям, каждая из которых соответствует аналогичному тесту. 

Память.

Тест 1 Sec.



Рис.11. Win 7 – память, тест 1 sec

















Рис.11.1. Win 7_2 – память, тест 1 sec





Рис.11.2. Win 7_2 – память, тест 1sec





Рис.11.3. Win 7 – память, тест 1 sec





Рис.11.4. Win 7 – общая статистика, тест 1 sec 



Рис.11.5. Win 7_2 – общая статистика, тест 1 sec



Из рисунков видно, что в случае, когда задачи запускаются с интервалом в одну секунду одновременно на обеих ВМ, тогда, как виртуальные машины, так и система в целом испытывают на себе высокую нагрузку. 




Тест 5 sec



Рис.12. Win 7 - память, тест 5sec



Рис.12.1. Win 7 - память, тест 5sec








Рис.12.2. Win 7_2 - память, тест 5sec





Рис.12.3. Win 7_2 - память, тест 5sec






Тест 30 sec





Рис.13. Win 7_2 - память, тест 30 sec





Рис.13.1. Win 7_2 - память, тест 30 sec






Рис.13.2. Win 7 - память, тест 30 sec





Рис.13.3. Win 7- память, тест 30 sec

Так как при данном тесте на виртуальной машине Win 7 приложения запускались с интервалом в 30 секунд и временем жизни 1 минута, а на виртуальной машине Win7_2 с интервалом 1 секунда и бессрочным сроком существования, нагрузка на Win 7 отказалась существенно меньше, что мы можем наблюдать на рисунках выше.

Тест 1 min



Рис.14. Win 7 - память, тест 1 min





Рис.14.1. Win 7 - память, тест 1 min

Ситуация примерно такая же, как и в случае с 30 секундами. Существенных различий не наблюдается, так как виртуальная машина Win7_2 забирает свободные ресурсы для своих задач. Отображаемая нагрузка на Win 7 не совсем корректна, так как в ней учитывается отданная доля другой виртуальной машине.

Тест 5 min



Рис.15. Win 7 - память, тест 5 min



Рис.15.1. Win 7_2 - память, тест 5 min

В процессе данного теста была выявлена наименьшая нагрузка как на виртуальные машины, так и на систему в целом. Данные результаты показывают, что когда одна из виртуальных машин почти не использует ресурсов, вторая виртуальная машина забирает свободные ресурсы для себя. В результате, система испытывает минимальную нагрузку.

Процессор

Тест 1 sec



Рис.16. Win 7 - процессор, тест 1 sec



Рис.16.1. Win 7_2 - процессор, тест 1 sec



Рис.16.2. Win 7_2 - процессор, тест 1 sec



Рис.16.3. Win 7 - процессор, тест 1 sec

Даже при высокой интенсивности запросов к виртуальным машинам, процессор функционирует в нормальном режиме и нагрузка не превышает 40%, что является очень хорошим результатом.




Тест 5 sec



Рис.17. Win 7 - процессор, тест 5 sec



Рис.17. Win 7_2 - процессор, тест 5 sec

При незначительном увеличении временного интервала запросов, нагрузка на процессор снизилась в несколько раз.




Тест 30 sec



Рис.18. Win 7 - процессор, тест 30 sec



Рис.18.1. Win 7 - процессор, тест 30 sec








Рис.18.2. Win 7_2 - процессор, тест 30 sec



Рис.18.3. Win 7_2 - процессор, тест 30 sec

Скачок нагрузки на процессор в данном случае обусловлен тем, что виртуальная машина Win7_2 забрала часть процессорных ресурсов для своих нужд. В целом, когда непредвиденная ситуация завершилась, загрузка процессора уменьшилась до 1-2%.




Тест 1 min



Рис.19. Win 7 - процессор, тест 1 min



Рис.19.1. Win 7_2 - процессор, тест 1 min

Как и предполагалось, виртуальная машина Win 7 почти не потребляет процессорных ресурсов, ввиду большого интервала выполнения задач и малого срока жизни.



Тест 5 min



Рис.20. Win 7 - процессор, тест 5 min



Рис.20.1. Win 7_2 - процессор, тест 5 min

Нагрузка на виртуальный процессор Win 7 находится на уровне 1-2% на всем протяжении текущего эксперимента, что очевидно при малой интенсивности потока запросов. Win7_2 выполняет задачи на процессоре в штатном режиме и потери быстродействия обнаружено не было.

Диск

Тест 5 sec



Рис.21. Win 7 – диск, тест 5 sec



Рис.21.1. Win 7 – диск, тест 5 sec

Диск является одним из самых загруженных элементов системы наравне с памятью. Основная потеря быстродействия виртуальных машин происходит из-за активного использования файла подкачки виртуальными машинами. Именно эту ситуацию мы наблюдаем при высокой интенсивности потока запросов в систему.

Тест 30 sec



Рис.22. Win 7 – диск, тест 30 sec



Рис.22.1. Win 7_2 – диск, тест 30 sec

Как и в предыдущем случае, нагрузка на диск до сих пор велика несмотря на снижение интенсивности потока запросов в систему. Однако, работоспособность системы находится на нормальном уровне.

Тест 1 min



Рис.23. Win 7 – диск, тест 1 min



Рис.23.1. Win 7_2 – диск, тест 1 min

Нагрузка на жесткий диск является самым труднооптимизируемым параметром системы. Решением данной проблемы может стать увеличения физической оперативной памяти сервера, а также установка дополнительных жестких дисков. 



Тест 5 min



Рис.24. Win 7 – диск, тест 5 min



Рис.24.1. Win 7_2 – диск, тест 5 min

Как видно из рисунков, при интервале запросов 5 минут на виртуальной машине Win 7, нагрузка на жесткий диск относительно стабильна. Суть проблемы в том, что виртуальные машины используют виртуальные жесткие диски, расположенные на одном физическом. 

Общая нагрузка

Тест 1 sec



Рис.25. Win 7_2 – общие показатели, тест 1 sec



Рис.25.1. Win 7 – общие показатели, тест 1 sec







Нагрузка на сервер (память)



Рис.26. Сервер – память, тест 1 sec

Нагрузка на сервер (Процессор)



Рис.26.1. Сервер - процессор, тест 1 sec





Рис.27. Общие показатели, тест 1 sec

Тест 5 sec



Рис.28. Общие показатели, тест 5 sec



Рис.28.1. Win 7 - Общие показатели, тест 5 sec



Рис.28.2. Win 7_2 - Общие показатели, тест 5 sec




Тест 30 sec



Рис.29. Win 7 - Общие показатели, тест 30 sec



Рис.29.1. Win 7_2 - Общие показатели, тест 30 sec






Рис.29.2. Win 7 - Общие показатели, тест 30 sec



Рис.29.3. Win 7_2 - Общие показатели, тест 30 sec






Рис.29.4. Общие показатели, тест 30 sec

1 min



Рис.30. Win 7 - Общие показатели, тест 1 min

Значение 102% нагрузки на память, как говорилось ранее, не является реальным, а учитывает отданные другой ВМ ресурсы.





Рис.30.1. Win 7_2 - Общие показатели, тест 1 min



Рис.30.2. Общие показатели, тест 1 min




Тест 5 min



Рис.31. Win 7 - Общие показатели, тест 5 min



Рис.31.1. Win 7_2 - Общие показатели, тест 5 min






Рис.31.2. Общие показатели, тест 5 min



Рис.31.3. Общие показатели, тест 5 min






Рис.31.4 Win 7 - Общие показатели, тест 5 min



Рис.31.5. Win 7_2 - Общие показатели, тест 5 min




Из графиков (рис.11 – рис.31.) видно, что процессор в течении тестирования не удалось загрузить на максимальные значения, однако свободной памяти почти не осталось и виртуальные машины встали в очередь за ресурсами, которые гипервизор выделял в процессе освобождения. Так же можно наблюдать, что самым загруженным компонентом системы является виртуальный жесткий диск, так как при достижении предела выделенной физической памяти, виртуальные машины начинают активно использовать файл подкачки. После снижения интенсивности нагрузки на виртуальные машины, виртуальные машины стали функционировать в обычном режиме (Рис.32. и Рис.32.1.).



Рис.32. Снижение интенсивности нагрузки, память



Рис.32.1 Снижение интенсивности нагрузки, процессор

Важно отметить, что первоначально ресурсы сервера, доступные обеим виртуальным машинам, не были ограничены совсем. В данном случае, виртуальные машины не испытывали проблем с быстродействием, но сервер получал ощутимую нагрузку, вследствие чего производительность системы в целом была удовлетворительная. 

Так же на начальном этапе тестирования не были заданы веса ресурсов для всех виртуальных машин. Это приводило к тому, что при нехватке ресурсов, гипервизор не перераспределял ресурсы между виртуальными машинами, а предоставлял их за счет мощностей сервера. Данный механизм нам совершенно не подходит из-за чрезмерной нагрузки на сервер, которой в свою очередь должен выполнять собственные задачи, помимо поддержания работоспособности в нашей облачной системе. Вследствие этого, гипервизор был настроен на ограничение использования ресурсов сервера виртуальными машинами. Подробнее о последующих конфигурациях облачной системы будет описано в следующих разделах.

Стоит заметить, что в условиях реального использовании системы клиентами, не происходит столь активного использования приложений на виртуальных машинах. Так что, с учетом этого, система не должна получать столь значительных нагрузок. Но целью наших тестов было выяснить, как ведет себя гипервизор и гостевые ОС в ситуации перегрузок.

В процессе тестирования механизма распределения ресурсов гипервизора был проведен тест, при котором запускалось по 3-4 приложения на каждой машине с различными интервалами во времени. В условиях столь низкой нагрузки, виртуальные машины использовали всего лишь 30% имеющихся в их распоряжении ресурсов. В момент, когда количество приложений и интенсивность их использования и запуска существенно увеличивалась, каждая из виртуальных машин была загружена на 95%. Эти цифры не означают, что система зависала и переставала работать. В этом случае быстродействие понижалось, но функционирование системы не было нарушено. Процессы просто ожидали освобождения ресурсов, и выполнялись когда таковые были выделены гипервизором. Важно уточнить, что помимо пользовательских приложений, виртуальная машина требует ресурсов для своего функционирования – запуска и поддержания работы операционной системы, системных служб и фоновых приложений. Затраты мощностей на данные задачи в нашем случае (ОС windows 7 -  требует 512 МБ для запуска и примерно 400 МБ в процессе функционирования в зависимости от количества включенных служб и фоновых системных приложений) составляют примерно 30% от выделенных ей ресурсов. Далее приведено изображение нагрузки на виртуальную машину Win 7 при больших интервалах запуска приложений (Рис.33.).





Рис.33. Малая нагрузка на виртуальную машину

Как видно из изображения, виртуальный процессор работает всего на 1% от имеющейся мощности, а память используется всего на 50% (550/1000 МБ). Отображаемый 101% нагрузки не является реальной нагрузкой на данную ВМ. В данном случае, приложения, функционирующие на Win 7 резервируют память «на запас», а то время как на самом деле, не являются активными и ресурсы им не требуются. В это время на виртуальной машине Win7_2 происходит выполнение нескольких программ, и гипервизор перераспределил ресурсы виртуальных машин в сторону Win7_2. После завершения работы приложений на Win7_2, ОС на Win7_2 выгрузила ненужные ей ресурсы и отдала их под управление гипервизора (Рис.34).



Рис.34. Приватизация ресурсов виртуальной машиной. Сравнить с Рис.33.

В процессе работы выкокой нагрузки на виртуальный коммутатор и сеть выявлено не было. 

Данные тесты позволили проверить механизм управления виртуальной системой гипервизором.

Анализ результатов тестов производительности

Управление и организация работы Hyper-V проводилась с помощью инструментов Windows PowerShell, в том числе установленного дополнительно модуля команд для гипервизора PS Hyper-V R2 [5,6]. Дополнительно к этому использовалась среда отладки и выполнения команд Windows Power Shell ISE, в которой описаны все возможные команды, их аргументы и параметры. Данный программный модуль позволил быстро и точно задать нужные параметры для виртуальных машин, сервера, и ресурсов. Так же для этих целей использовался Hyper-V Manager, однако удобство графического интерфейса настройки гипервизора и системы ограничено отсутствием некоторых настроек, доступных только из командной строки.

В ходе экспериментов были выявлены такие проблемы как потеря производительности виртуальных машин, уменьшение быстродействия сервера. Далее о каждой проблеме будет описано подробнее.

На начальном этапе, ресурсы, доступные виртуальным машинам ограничены не были. Это приводило к высокой нагрузке на сервер, вследствие чего были выставлены некоторые ограничения.

Различные веса ресурсов для виртуальных машин были выставлены с целью изучения механизма распределения ресурсов и дальнейшей оптимизации системы.

Память

Для win 7 вес памяти - высокий , win7_2  - ниже среднего. Буфер памяти в обоих случаях – 10%.

Процессор

Для Win 7 резерв ресурсов процессора - 5% от общих системных ресурсов, вес ресурсов - 150, для Win7_2 – аналогичные параметры, однако вес ресурсов установлен 100. Предел использования ресурсов процессора – 30% от мощности сервера для каждой из ВМ.

Данные изменения привели к тому, что система стала работать быстрее, а также мы смогли проанализировать взаимодействие виртуальных машин. 

После изменений параметров гипервизора снова были проведены те же тесты, которые показали следующий результат:

Результаты повторного тестирования для некоторых параметров системы приведены на рис.35. и рис.36.









Рис.35. Результат оптимизации работы гипервизора, Win 7, тест 1 sec

Win7_2, 1 sec интервал









Рис.35.1. Результат оптимизации работы гипервизора, Win 7_2, 1 sec









Рис.36. Результат оптимизации работы гипервизора, Win 7, тест 5 min

Win7_2, 5 min интервал











Рис.36.1. Результат оптимизации работы гипервизора, Win 7_2, тест 5 min

В ходе тестирования было выявлено, что виртуальная машина Win 7 имеющая более высокий вес ресурсов, получает их быстрее и выгрузка ресурсов из неё происходит медленнее. Также Win 7 в несколько раз больше проводит времени в привилегированном режиме работы. Это означает, что она чаще получает контроль над элементами управления системой облачных вычислений для выполнения требуемых операций. 

В то же время, если у Win 7 освобождаются ресурсы, которые требуются другой виртуальной машине, она отдает их гипервизору, который в свою очередь, решает куда перенаправить их дальше. 

Во время тестирования было выявлено, что в процессе работы виртуальных машин, при больших нагрузках на них, происходит перегрузка жесткого диска на сервере, так как виртуальные машины и сам сервер используют помимо физической памяти, виртуальную. Виртуальные машины при недостатке выделенной памяти начинают активно использовать файл подкачки, расположенный на виртуальных жестких дисках, связанных с физическим на сервере. Скорость доступа к диску и обработка данных на нем существенно меньше скорости оперативной памяти. Вследствие этого, система теряет своё быстродействие в несколько раз, как видно на рис.37.



Рис.37. Перегрузка жесткого диска на виртуальной машине

Для решения данной проблемы пришлось ограничить файл подкачки на виртуальных машинах почти в 2 раза. На обоих ВМ его размер составил 512 мб.

Также для увеличения быстродействия виртуальных машин было задано  соответствие между самой машиной и виртуальными процессорами. Каждая из ВМ получила в своё распоряжение по 2 различных логических процессора сервера. 

Сервер

Для сервера было зарезервировано 500 МБ оперативной памяти, доступ к которой не могут получить виртуальные машины. Данное ограничение задано для того, чтобы сервер мог корректно функционировать при высокой нагрузке на систему, так как виртуальные машины могут забрать нужные серверу ресурсы для корректной работы. Для этого в разделе реестра:

HKLM\SOFTWARE\Microsoft\WindowsNT\CurrentVersion\Virtualization создан резерв памяти (ключ типа DWORD с названием MemoryReserve) 

Жесткие диски

Жесткие диски на ВМ заданы в формате .vhdx – динамически расширяемые жесткие диски. Размер 20гб. и 30гб. для двух разных виртуальных машин. Особенность данного формата в том, что при создании жесткого диска, в виртуальной машине он отображается как обычный жесткий диск с фиксированным размером, а в на физическом жестком диске занимает фактических размер ( который сейчас занят, не предельный заданный). Это помогает сэкономить место на оборудовании. Скорости работы статического виртуального жесткого диска и динамического почти одинаковые, поэтому мы используем более удобный второй вариант.

Сеть.

При построении сети использовался виртуальный коммутатор и тип сети определен как внешняя. В нашем случае доступ в интернет предоставляется серверу через беспроводные сети Wi-fi. Виртуальный коммутатор полностью «занимает» физическую сетевую карту сервера и занимается сетевым взаимодействием между виртуальными машинами, сервером и внешней сетью. В данном случае виртуальные машины могут свободно взаимодействовать с хостовой ОС. Между ВМ и серверам была организована локальная сеть.  Также выставлена защита маршрутизатора для защиты от сообщений и перенаправлений от несанкционированных виртуальных машин которые выдают себя за марщрутизаторы.

Возможность построения сети с помощью беспроводного адаптера Wi-Fi является несомненным плюсом гипервизора Client Hyper-V. Изначально, сеть на сервере была настроена через сетевую карту Ethernet и кабеля типа витая пара. В дальнейшем сеть была перенастроена с помощью адаптера Wi-Fi. При этом уменьшение быстродействия сети не наблюдалось. При условии высокой скорости передачи данных по сети, использование Wi-Fi при развертывании облачной системы с помощью Client Hyper-V является отличным решением. 

Для взаимодействия виртуальных машин, а в особенности обмена данными по локальной сети, была организована домашняя группа с общим доступом к  файлам на каждой из ВМ, а так же общее использование принтеров и других инструментов.

Общее

В процессе тестирования было выявлено, что при грамотном распределении ресурсов сервера, хостовая операционная система Windows 8 не испытывает проблем с быстродействием даже при большой нагрузке на виртуальные машины (облачную систему). В случае, если гипервизор, работающий на родительской ОС, не настроен на ограничение распределения ресурсов, облачная система забирает почти все имеющиеся у сервера ресурсы и в системе возникают перегрузки. 

В случае, если в системе отключается динамическая память и задается статическая, гипервизор начинает использовать технологию неоднородного доступа  к памяти NUMA. Она заключается в том, что при многопроцессорной архитектуре процессоров n-ое количество (в нашем случае – два), а память одна. Таким образом, часть процессоров простаивает, в ожидании, пока они смогут обратиться к памяти. NUMA выделяет каждому процессору свою память. Между узлами данной памяти прокладывается быстрая шина, через которую процессор может обращаться к «чужой» для его памяти своих соседей. Получается ситуация, когда память физически распределена, но логически общедоступна. Недостатком данного метода является то, что обмен данными между узлами NUMA [40] меньше скорости обмена процессоров и памяти в обычном режиме.

В процессе тестирования было замечено, что в случае, когда ядер в процессоре  n<4 и логических процессоров n<8, а также объем оперативной памяти меньше 8 ГБ.,  архитектура NUMA не имеет результата, так как процессор справляется и так. В нашем случае мы сделали выбор в пользу динамической памяти, так как с её использованием повышается общая производительность системы. 

При наличии на сервере большего количества виртуальных машин чем логических процессоров сервера, производительность системы незначительно падает, но при условии наличия большого количества оперативной памяти на оборудовании сервера, быстродействие почти не страдает, так как современные процессоры могут обрабатывать огромное количество операций в секунду. Опытным путем было выявлено, что при текущей конфигурации сервера, при наличии не двух, а шести виртуальных машин, ощущается потеря быстродействия системы из-за нехватки памяти на 50%. Быстродействие процессора в то же время упало всего на 20%. Также потеря быстродействия ощущается из-за всего одного доступного физического жесткого диска, так как вся система, в том числе и сервер, используют виртуальную память и файл подкачки. При наличии нескольких жестких дисков и большего объема оперативной памяти, быстродействие системы значительно повышается.

Выводы

 В данном разделе были проведены эксперименты над «облачным стендом», результаты которых были приведены в предыдущих разделах. Проанализировав полученные результаты, были приняты меры по организации работы гипервизора. Как результат, эффективность и быстродействие системы увеличилось. В целом, если распределение ресурсов проводится гипервизором в автоматическом режиме, без организации его работы, система может функционировать, но, при достижении высоких нагрузок, виртуальные машины потребляют все больше и больше ресурсов. В случае, когда на сервере работает большое число виртуальных машин, такая ситуация может привести к критическому исходу, в результате чего работоспособность всей системы может быть нарушена. Следовательно, требуется точная настройка всех параметров под каждого конкретного клиента. Ресурсы должны быть масштабируемы в зависимости от потребностей пользователя облака. В данном разделе были описаны все параметры, значения которых должны быть обязательно заданы при организации работы гипервизора вне зависимости от типа используемого гипервизора и модели. Также были описаны основные средства, с помощью которых можно проводить мониторинг системы, её отладку и настройку.

В заключение, перечислим параметры, которые были заданы для виртуальных машин в ходе выполнения проекта:

Win 7.

Память: 

Оперативная память (ОЗУ для запуска): 512МБ 

Минимальный объем оперативной памяти: 50МБ

Максимальный объем оперативной памяти: 1000МБ

Буфер памяти: 10%

Вес памяти: высокий

Процессор: 

Виртуальных процессоров: 2

Резерв для виртуальных машин: 10%

Процент объема от системных ресурсов: 5%

Предел ресурсов для виртуальных машин: 60%

Процент от общего объема системных ресурсов: 30%

Относительный вес процессорных ресурсов: 150

Жесткий диск:

Размер виртуального жесткого диска: 20ГБ

Размер файла подкачки: 512МБ

Win 7.

Память: 

Оперативная память (ОЗУ для запуска): 512МБ 

Минимальный объем оперативной памяти: 50МБ

Максимальный объем оперативной памяти: 1000МБ

Буфер памяти: 10%

Вес памяти: низкий

Процессор: 

Виртуальных процессоров: 2

Резерв для виртуальных машин: 10%

Процент объема от системных ресурсов: 5%

Предел ресурсов для виртуальных машин: 60%

Процент от общего объема системных ресурсов: 30%

Относительный вес процессорных ресурсов: 100

Жесткий диск:

Размер виртуального жесткого диска: 30ГБ

Размер файла подкачки: 512МБ

Сеть:

Ввиду малого количества виртуальных машин, полоса  пропускания  сети изменению не подлежат. Сетевое взаимодействие осуществляется гипервизором в автоматическом режиме. Однако для каждой виртуальной машины имеется возможность ограничить полосу пропускания на конкретные значения Мбит/с.


Глава 4. Математическая модель гипервизора

В данном разделе приводятся результаты построения математической модели для анализа работы гипервизора при планировании работы виртуальных машин.

Приводятся результаты расчетов по моделям.

Описание системы

Существует мнение, что облачные системы позволяют повысить производительность оборудования, и виртуализация ускоряет работу системы в целом. Однако это не так. Облачные технологии позволяют осуществить удобный и быстрый доступ к вычислительным ресурсам оборудования, на котором они функционируют, а также снизить затраты на создание сетевой инфраструктуры, но они никаким образом не могут повысить производительность физического оборудования, так как основа облачных технологий - гипервизор всего лишь управляет ресурсами, а не увеличивает их количество.

Важной задачей гипервизора является выделение ресурсов процессора облачного сервера (процессорного времени) виртуальным машинам. При этом, естественно, от количества выделенного времени зависит эффективность работы всей облачной системы, поскольку недостаток ресурсов приводит к возникновению очередей из обрабатываемых на виртуальных машинах запросов, и, задержкам в обслуживании пользователей.

В процессе работы гипервизор имеет возможность варьировать ресурсы выделяемые различным виртуальным машинам. При этом, поскольку он работает как операционная система, то процессорное время выделяется виртуальным машинам в виде квантов. Количество выделенных каждой машине квантов может быть различным и от этого зависит вычислительная мощность (производительность) виртуальных машин.

Для анализа влияния плана распределения ресурсов процессора построим математическую модель.

Модель работы системы

В качестве параметра модели будем рассматривать варианты распределения процессорного времени между виртуальными машинами. Данный параметр отвечает за непосредственное выполнение задач системы на физическом процессоре. 

Рассмотрим произвольный интервал времени работы системы – T.

Если существует только один виртуальный сервер и длина кванта времени, выделяемого  этому серверу – z., то количество выделенных квантов для решения задач сервера - .

Если существует N виртуальных машин, тогда за время Т каждая ВМ получит   квантов времени для каждой. N+1 показывает, что кроме ВМ квант процессорного времени выделяется на управление гипервизору.

Рассмотрим базовое отношение . Это означает, что время, выделяемое в случае  в N+1 раз больше. 

На рисунке 38 показана зависимость выделения квантов времени виртуальным машинам от их количества.



Рис.38. График выделения времени

Как видно из графика (рис.38.), при увеличении числа виртуальных машин, количество времени, выделяемого каждой ВМ уменьшается.

Поскольку гипервизор может управлять процессом выделения процессорного времени виртуальным машинам рассмотрим следующий случай.

Пусть  – интенсивность потока запросов (число запросов в единицу времени) в систему. Цикл работы гипервизора (план) состоит из тактов (квантов процессорного времени), выделяемых каждой ВМ плюс один квант для сервера.  Пусть  – число квантов времени для i-й ВМ. – интенсивность потока запросов i-й ВМ.

Эффективность работы гипервизора при плане (n1,n2,…,nN) вычисляем по формуле:

, где N – число ВМ,  – коэффициент приоритета виртуальных машин (задает вес ресурсов конкретной виртуальной машины). Такой показатель эффективности позволяет оценить затраты, связанные с ожиданием запросов в очередях к виртуальным машинам.

Длина цикла в квантах в данном случае будет равна: 

.

Сумма интенсивностей  потоков запросов на каждую ВМ будет равна интенсивности общего потока запросов в систему.

,

 где λi – интенсивность потока запросов на i  ВМ.

Доля процессорного времени, выделяемого i-й ВМ :

.

При этом   – интенсивность обслуживания запросов i-й ВМ.

Здесь µ0 – интенсивность обслуживания запросов в случае, когда в облачной системе только один сервер.

Среднее время ожидания в очереди при наличии только одного сервера вычислим, полагая, что в качестве модели сервера используется СМО типа M/V/1/∞ []: 



Среднее время ожидания клиента в очереди к серверу номер i, когда в системе несколько виртуальных серверов:

.

При этом . Это означает, что запросы обрабатываются быстрее, чем поступают в систему.

Среднее ожидание клиента в очереди с учетом предыдущих обозначений будет следующим:



Параметр утилизации (загруженности виртуального сервера i) равен: 

.

Длина очереди запросов к серверу i равна:

 .

Эффективность работы одного сервера:

 

Общая эффективность системы:

 , где N – число ВМ,  – коэффициент приоритета виртуальных машин (задает вес ресурсов конкретной виртуальной машины).

 Расчеты по модели

Рассчитаем числовые значения по приведенным формулам.

Пусть будут заданы следующие параметры:

, , 

Результаты моделирования приведены в таблице 1.





Результаты расчетов





Как видно из таблицы 1, при увеличении числа квантов времени, выделяемых для виртуальных машин, эффективность системы увеличивается. Но при этом не стоит задавать слишком большое число квантов, так как увеличится длина общего цикла, что скажется не лучшим образом на производительности системы.

Кроме того, наиболее часто применяемый план работы гипервизора, приведенный в первой строке таблицы не всегда наиболее эффективен.

Выводы

В данном разделе были проведены расчеты выделения гипервизором процессорного времени виртуальным машинам по математической модели. Проанализировав данные результаты, был сделан вывод о целесообразности более тонкой настройки параметров распределения процессорных и других ресурсов гипервизором. Результаты моделирования полностью совпадают с теми результатами, которые были получены в ходе экспериментов на «облачном» стенде. Стоит отметить, что выделение чрезмерного количества ресурсов одному из клиентов системы без необходимости, приведет к простаиванию системы, что скажется не лучшим образом на общей производительности, а также быстродействии других клиентов системы.




Глава 5. Охрана труда

Защитное зануление

Занулением [11] называется электрическое соединение металлических нетоковедущих частей электроустановок с заземленной нейтралью вторичной обмотки трехфазного понижающего трансформатора (генератора), которые могут оказаться под напряжением. Зануление применяется в трехфазной сети с заземленной нейтралью напряжением до 1000В. В таких сетях нейтраль источника тока присоединена к заземлителю с помощью заземляющего проводника. Этот заземлитель располагается вблизи источника питания или около стены здания, в котором он находится. [11]

В сети с занулением различают нулевой защитный проводник(НЗ) и нулевой рабочий проводник(НР). Нулевым защитным проводником называется проводник, соединяющий зануляемые части с заземленной нейтральной точкой обмотки источника тока или его эквивалентом. Нулевой рабочий проводник используют для питания током электроприемников и тоже соединяют с заземленной нейтралью трансформатора или генератора.

Защита человека от поражения электрическим током в сетях с занулением осуществляется тем, что при замыкании одной из фаз на зануленный корпус в цепи этой фазы возникает ток короткого замыкания, который воздействует на токовую защиту (плавкий предохранитель, автомат), в результате чего происходит отключение аварийного участка цепи. Кроме того, еще до срабатывания защиты, ток короткого замыкания вызывает перераспределение напряжений в сети, приводящее к снижению напряжения корпуса относительно земли. Таким образом, зануление уменьшает напряжение прикосновения и ограничивает время, в течение которого человек, прикоснувшийся к корпусу, может попасть под действие напряжения [11].

Защитное действие зануления обеспечивается надежным срабатыванием максимальной токовой защиты на быстрое отключение участка цепи с поврежденной изоляцией. Для этого необходимо, чтобы ток короткого замыкания в цепи фаза-нуль отвечал условию , где k – коэффициент надежности (качества), - номинальный ток отключающего устройства (автомат, предохранитель).

Коэффициент надежности k согласно ПУЭ должен быть:

Для тепловых автоматов и предохранителей – 3

Для автоматов с электромагнитным расцепителем – 1.4

Для взрывоопасных помещений – 4-6

Проводимость магистрального нулевого защитного проводника, присоединенного к нейтрали источника питания, должна быть не ниже 50% проводимости фазных проводников. Этим обеспечивается его стойкость на нагрев при прохождении тока однофазного короткого замыкания. 

Тепловые автоматы

Тепловой автомобильный выключатель (автомат) надежно защищает силовые цепи от коротких замыканий. Также тепловой автомат применяется в цепях с большими пусковыми токами: электромоторы, компрессоры и т.п.

 При срабатывании, т.е. коротком замыкании или долгосрочном превышении номинальной нагрузки тепловой автомат размыкает цепь.

Тепловой расцепитель в автоматическом выключателе представляет собой биметаллическую пластину, нагреваемую протекающим током. При протекании тока выше допустимого значения биметаллическая пластина изгибается и приводит в действие механизм расцепления. Время срабатывания зависит от тока и может изменяться от секунд до часа. Минимальный ток, при котором должен срабатывать тепловой расцепитель, составляет 1,45 от номинального тока предохранителя. В отличие от плавкого предохранителя, автоматический выключатель готов к следующему использованию после остывания пластины.

Расчет защитного зануления

Расчет зануления состоит в выборе нулевого защитного проводника (материал, площадь поперечного сечения, способ прокладки) и последующим определением тока однофазного короткого замыкания. Необходимо определить сопротивление отдельных участков электрической цепи и по ним суммарное полное сопротивление цепи.

Порядок определения полного сопротивления  следующий:

Определить активное сопротивление фазных, ответвляющих и нулевых защитных проводов в питающей среде по формуле:

, где  – удельное сопротивление материала проводника, Ом*м; 

λ – длина проводника, м;

S – площадь поперечного сечения проводника, мм2.

λ 1 – фазный провод, м;

λ 2 – провод ответвления, м;

λ 3 – НЗП, м.

Определить полное сопротивление петли фаза-нуль (rn)

rn=r1+r2+rнзп, где  ;  ;  .

Определить возможный ток короткого замыкания



Определить по Iкз Iном необходимого автомата для включения в цепь питания ЭЛУ





Возьмем следующие значения:

Тип сети: 380/220В

Потребляемая мощность: 400Вт

Расстояние от питающего трансформатора: 750м

Материал фазных проводников: алюминий

Расстояние от потребителя до фазного провода: 100м

Длина НЗП: 50м

Площадь поперечного сечения НЗП: 0,5мм2 

Материал НЗП: медь

Удельное сопротивление НЗП: 0,0175 Ом

Тип автомата: тепловой

Сопротивление обмотки трансформатора: 0,412 Ом

Площадь поперечного сечения фазных проводников: 3мм2

Результаты расчетов защитного зануления: 

r1 = 0.093 ОМ

r2 = 7 ОМ; r3=1.75 ОМ; rn = 8.843 ОМ

Iкз =44А

Iном=14,6А



Выводы

В данной главе был рассмотрен вопрос об охране труда при выполнении работ по организации работы гипервизора. По результатам анализа проблемы, для увеличения безопасности труда рекомендуется использовать защитное зануление сервера, которое позволит снизить риск удара электрическим током при взаимодействии с оборудованием сервера. Также данная мера позволит не допустить выход оборудования из строя в следствии возникновения неполадок в сетях с напряжением до 1000В, а также в иных критических ситуациях, связанных с электричеством.




Глава 6. Экономическая часть

Анализ рынка

В настоящее время легко заметить тенденцию к активному использованию облачных технологий и вычислений. Многие как крупные, так и небольшие предприятия переходят от классического подхода построения ИТ - инфраструктуры к облачному. Для динамически развивающихся компаний выгоднее приобретать облачные сервисы взамен физического оборудования и программного обеспечения. До появления «облаков», затраты на развертывание сетевой инфраструктуры компаний порой достигали огромных размеров. Помимо покупки мощного физического оборудования, на одном из первых мест стояла проблема приобретения лицензированного программного обеспечения - корпоративные лицензии по стоимости превышают розничные в несколько раз. С появлением облачных технологий, а вместе с ними облачных поставщиков услуг, данные проблемы получили абсолютно новое решение. Сейчас существует возможность арендовать ИТ-инфраструктуру и вычислительные мощности у облачного провайдера, которые могут быть масштабированы в любой момент как в сторону увеличения, так и в сторону уменьшения без высоких затрат. Важно заметить, что при данном подходе самое разнообразное программное обеспечение включено в аренду, а с поставщиками облачных услуг в настоящее время сотрудничают большинство разработчиков программного обеспечения. 

В то же время, поставщик облачных услуг должен организовать собственную систему таким образом, чтобы обеспечить клиентов своевременным высококачественным доступам к вычислительным ресурсам и инфраструктуре по первому требованию. Современные облачные системы имеют очень сложную архитектуру, состоят из множества серверов, ЦОДов и других элементов. Если рассмотреть облако более детально, одним из основополагающих элементов является гипервизор. Данный элемент отвечает за постоянное перераспределение физических ресурсов оборудования серверов между виртуальными клиентами системы. Именно от организации работы гипервизора зависит эффективность облачной системы. 

В настоящий момент на российском рынке существует несколько решений, которые предлагают западные компании. К сожалению, отечественных гипервизоров в продаже не существует. Среди самых распространенных гипервизоров стоит отметить Microsoft Hyper-V/client Hyper-v, VMware ESXi/Workstation, KVM, Xen Server, помимо которых существуют другие решения. 





Сопоставим стоимость гипервизорных решений:

Hyper-V: Входит в состав Microsoft Windows Server 2012 – от 1120 до 250 000 руб. Цена зависит от количества подключаемых виртуальных клиентов, количества процессоров на сервере и т. д.

VMware vSphere(ESXi) – продается отдельно как ОС, цена от 3180 до 170000 руб. Как и в предыдущем случае зависит от количества клиентов, процессоров, а также срока использования ( от 1 года до безлимитной)

Xen Server – продается как отдельное серверное ПО, цена от 32000 до 1 280 000 руб. Цена зависит от количества серверов, их конфигурации. В стоимость включена подписка на 1 год.

Client Hyper-V: входит в состав Windows 8 Pro, цена которого от 3000 до 9 000. Зависит от типа лицензии (корпоративная, частная).

Стоимость прикладного ПО, использующегося на сервере, а также операционных систем на виртуальных машинах оплачиваются отдельно исходя из потребностей.

Расчет стоимости проекта

Затраты на проект по организации работы гипервизора определяются по себестоимости. Стоимость построения собственного облака рассчитана не будет, так как нас интересует лишь один элемент облачной системы – гипервизор. 

Для проведения конкретных расчетов, разделим проект на следующие этапы:

Составление технического задания(1 день)

Анализ гипервизорных решений, а также особенностей будущей облачной системы(10 дней)

Принятие решения по выбору средств реализации поставленной задачи (1 день)

Покупка требуемого серверного ПО(1 день)

Установка и настройка гипервизора(4 дня)

Установка необходимого ПО на сервер и виртуальные машины(5 дней)

Оформление технической документации(2 дня)

Итого на выполнение организации работы гипервизора при создании систем облачных вычислений требуется 25 рабочих день. С учетом выходных, на месяц приходится 21-22 рабочих дней. Зарплата специалистов по облачным технологиям составляет примерно 70 000 рублей в месяц. После успешного выполнения проекта, специалист получает бонус в размере 15 тыс.руб.

Рассчитаем з/п специалиста в день:

ЗПдень= 70000/21=3333.3 руб/день

Тогда заработная плата специалиста за весь проект составит:

ЗП = ЗПдень*25 + бонус = 98333 руб.

Кроме затрат на заработную плату специалисту, существуют дополнительные расходы.

Транспортные расходы специалиста целиком ложатся на плечи заказчика проекта. Так как специалист передвигается на личном автомобиле Toyota Camry, расход топлива которого составляет 14л/100км бензина в городском цикле. Специалист живет на расстоянии 30 км от офиса, так что дорога из дома на работу и обратно составляет 60 км/день.  С учетом дополнительных передвижений, которые совершает специалист в течение дня по рабочим делам, ему выдана топливная карта с 300 литрами бензина. Литр бензина АИ-95 в среднем по столице стоит 32 руб. 50 коп.

ТР = 300*32.50=9750 руб. на весь проект

Расходы на мобильную связь. Для постоянной связи со специалистом, ему выдана корпоративная сим-карта с безлимитным тарифом. Размер абонентской платы сотовому оператору составляет 1500 руб/месяц.

МР = 1500 руб. на весь проект

Расходы на интернет. Для выполнения работ специалисту выделен отдельный канал с безлимитным трафиком и скоростью передачи данных 100 Мбит/с. Его стоимость составляет 7500 руб/мес. для юридических лиц и индивидуальных предпринимателей. 

ИР = 7500руб. на весь проект

Расходы на оборудование и ПО. Оборудование серверов заказчик уже приобрел заранее, так что в стоимость проекта оно включено не будет. Так как выбор пал на гипервизор Client Hyper-V, входящий в состав Windows 8 Pro, цена составит 7150 руб. за 1 бессрочную лицензию. Также требуется приобрести ОС для виртуальных машин в количестве 2шт. Лицензия Windows 7 Ultimate стоит 5690 руб., х2=11380 руб. Затраты на покупку офисного пакета MICROSOFT OFFICE 365 SMALL BUSINESS PREMIUM для установки на виртуальные машины составляет 6190 руб. Данный пакет содержит лицензии для 5 компьютеров сроком на 1 год каждая. Установка дополнительных программ оплачивается заказчиком по желанию.

Итого ПоР=24720 руб. 

Расходы на питание специалиста. Питание специалиста оплачивается работодателем. Специалист имеет право на один обеденный перерыв в сутки. Заказчик приобрел талоны на месяц на бизнес-ланч в соседнем к офису ресторане. Стоимость одного полного бизнес-ланча составляет 310 руб. С учетом скидки от владельца ресторана, 1 бизнес ланч стал стоить 270 рублей.

Итого ПитР=270*25=6750 руб. за весь проект

Прочие расходы. Заказчик учел непредвиденные расходы, на которые он выделил 10 000 рублей.

ПрР = 10 000 руб.

Иные собственные расходы, не связанные с рабочим процессом оплачиваются специалистом самостоятельно и не входят в стоимость проекта.

В итоге, себестоимость проекта равна:

СБ = ПитР+ПрР+ПоР+ИР+МР+ТР = 158 500 рублей.

Далее необходимо подсчитать налоги и взносы. В данном случае используется упрощенная форма налогооблажения. Ввиду отсутствия наемных работников, требуется внести взносы в ПФР и ММФОС. Сумма налогов и взносов не должна превышать 6%. Подоходный налог рассчитывается исходя из дохода без учета затрат.

Взносы в Пенсионный Фонд. ПФР составляют 26% от з/п работника, в данном случае 2*МРОТ*0,26=2706,6 руб.

Взносы в Федеральный Фонд Обязательного Медицинского Страхования ФФОМС составляют 3,1-5,1% и равны МРОТ*0,031=275,4 руб.

ФФОМС+ПФР=2982 руб.

Налог на доход (зарплата + затраты) не должен превышать 6%:

НД = 158 500*0,06 = 9510 руб.

Рентабельность = прибыль/(цена – налоги)=40%

Прибыль = 62011 рублей.

Цена проекта равна:

Ц=СБ + ПФР + ФФОМС+ПР = 226580 рублей.

Теперь построим диаграмму (рис. 39.) экономических показателей проекта для получения итоговой картины.



Рис.39. Диаграмма экономических показателей

Выводы

В результате выполнения экономической части проекта были выявлены основные финансовые показатели проекта, рассчитаны затраты на заработную платы, дополнительные расходы, взносы в пенсионный фонд и фонд медицинского страхования, а также налоги. В итоге была вычислена итоговая цена проекта. Также была составлена диаграмма экономических показателей проекта, из которой можно видеть прибыль специалиста в виде заработной платы за выполненный проект и выполненные работы по организацию работы гипервизора.




Глава 7. Надежность

Общие сведения

Надежность включает в себя следующие компоненты:

Безотказность 

Ремонтопригодность 

Сохраняемость 

Долговечность 

На основании анализа данных компонентов можно сделать выводы о качестве и надежности системы.  Для надежной работы системы требуется обеспечить соблюдение следующих правил:

На этапе проектирования:

Соблюдение требований стандартов и всех условий разработки

Выбор программного обеспечения в соответствии с условиями эксплуатации

Создание и наличие полной технической документации

Создание предварительного расчета вероятного поведения системы

На этапе пусконаладочных работ:

Установка и первичная настройка гипервизора

Установка и настройка серверного ПО, а также клиентских приложений на виртуальные машины

Организация работы гипервизора согласно предъявленным требованиям

Проведение тестирования нагрузки на систему

Анализ результатов и исправление ошибок

На этапе эксплуатации:

Обслуживание системы техническим специалистом с соответствующей квалификацией

Контроль доступа к системе и защита информации 

Проведение технического обслуживания системы для профилактики и предотвращения всевозможных проблем и отказов. 

Скорейшее исправление возникших неполадок в работе системы




Надежность для гипервизора.

Одно из основных свойств системы является работоспособность. Потеря работоспособности – отказ.

Основным и единственным критическим элементом разрабатываемой системы является гипервизор Client Hyper-V. Данный элемент(гипервизор) является единой точкой отказа, в результате чего при его отказе, из строя выходит вся система, как следствие – функционирование системы не возможно.  Вследствие этого рекомендуется иметь в наличии как минимум два сервера – основной и резервный, для поддержания функционирования системы в случае отказа одного из гипервизоров. 

Гипервизор MS Client Hyper-V является в своем роде частью операционной системы, управляющей виртуальными машинами и родительской операционной системой.  Поэтому надежность гипервизора можно считать как надежность для программного обеспечения сервера (ПО).

Надежность программного обеспечения напрямую зависит от его качественного написания. 

Существуют такие свойства ПО как:

Завершенность

Устойчивость

Восстанавливаемость/ремонтопригодность

Доступность/готовность

Завершенность

Завершенность  ПО – свойство программного обеспечения не попадать в состояния отказов вследствие ошибок и дефектов в программах и данных. Данное свойство ПО определяется через такие показатели как наработка на ошибку и степень покрытия ПО тестами функций и структуры программы. 

Устойчивость 

Устойчивость ПО  к дефектам и ошибкам – свойство ПО автоматически поддерживать заданный уровень качества функционирования при проявлениях дефектов и ошибок или нарушениях установленного интерфейса. (ссылка проверить)

Восстанавливаемость

Восстанавливаемость – свойство ПО в случае отказа возобновить требуемый уровень качества функционирования, а также поврежденные программы и данные.

В случае отказа, системе требуется перезапуск для продолжения качественного функционирования. Перезапуск системы составляет 1 минута, при этом данные, обрабатываемые системой на момент отказа потеряны не будут. 

Доступность и готовность

Доступность и готовность – свойство ПО  быть в состоянии выполнять требуемые функции в любой момент времени при заданных условиях использования.



Теперь составим график (рис.40) зависимости вероятности отказа системы от временного интервала.



Рис.40. График зависимости вероятности отказа от времени.

Вероятность отказа системы увеличивается с течением времени по экспоненциальному закону. Важно заметить, что вероятность отказа резко возрастает после одного года работы гипервизора. Для сокращения вероятности отказа следует обновлять программное обеспечение, а именно версию гипервизора согласно выпуску новых версий поставщиком услуг. Данное обновление положительно скажется на надежности системы.

Построим график (рис.41) зависимости вероятности безотказной работы от времени.



М – месяц

Рис.41. График зависимости вероятности безотказной работы от времени.

Выводы

В результате проведенных исследований надежности гипервизора был сделан вывод о работоспособности системы. Вероятность безотказной работы на протяжении более чем года работы остается выше 0.91, что является более чем хорошим показателем надежности гипервизора.

Несмотря на это, своевременное обновление версии гипервизора, увеличит вероятность безотказной работы как гипервизора так и системы в целом на более долгий срок. 

Также не стоит забывать о профилактических работах в системе, которые положительно скажутся на её надежности.




Заключение. Общие выводы.

Проведен анализ особенностей работы облачных систем, принципов их построения, состава, программных и аппаратных средств, показавший перспективность данного направления развития средств вычислительной техники при создании систем, обслуживающих большое количество пользователей в режиме удаленного доступа. 

Проведен анализ принципов построения и работы гипервизоров облачных систем, показавший, что гипервизоры являются основными средствами управления работы виртуальных машин и, следовательно, повышения эффективности всей облачной системы.

Создан стенд облачной системы, позволивший проводить эксперименты по управлению ресурсами облачной системы. Необходимость стенда обусловлена тем, что в литературе практически отсутствуют сведения о работы гипервизоров по управлению ресурсами, алгоритмы распределения ресурсов между виртуальными машинами. Эксперименты на стенде позволили понять основные принципы работы облачной системы и гипервизора.

Разработана модель для оценки работы гипервизора по управлению ресурсами процессора облачного сервера. Модель позволила оценить эффективность облачной системы в зависимости от плана выделения ресурсов виртуальным машинам.

В разделах, посвященных экономике, охраны труда и надежности решены вопросы расчета контура зануления сервера, расчета экономических показателей работы, расчета отказоустойчивости гипервизора.








Список использованной литературы



Питер Фингар. Dot.Cloud: облачные вычисления - бизнес-платформа XXI века. – М.: Аквамариновая Книга, 2011. – 256 с.

Джордж Риз. Облачные вычисления.- СПб.: БХВ,2011.-288с.

Дерек Мелбер. Групповая политика Windows. Ресурсы Windows Server 2008, Windows Vista, Windows XP, Windows Server 2003 / Пер. Наталья Сержантова/ Русская редакция, БХВ, 2012 г. – 544 с.

Рэнд Моримото. Microsoft Windows Server 2008 R2. Полное руководство. /  – М.: Вильямс, 2011.-1456 с.

Роберт Ларсон. Платформа виртуализации Hyper-V. Ресурсы Windows Server 2008 / Роберт Ларсон, Жаник Карбон - СПб.: Русская Редакция, БХВ, 2008.-800 с.

Тони Нортроп. Проектирование сетевой инфраструктуры Windows Server 2008. Учебный курс Microsoft /  Тони Нортроп, Дж. К. Макин- М. : Русская редакция,2012.-720с.

Михаил Михеев. Администрирование VMWare vShere 5.- М.: ДМК Пресс,2012.-504 с.

В.Г. Еремин, В.В. Сафонов. Методы и средства    обеспечения безопасности труда в машиностроении: учебник для вузов / под ред. Ю.М. Соломенцева – М.: Высшая школа, 2000.

Аледрей Колесов. Вернемся к нашим гипервизорам//PCWeek.-2009.-№16-17(670-671)

Крис Касперски. Властелин виртуальных машин: Практические советы по развертыванию виртуальной инфраструктуры // Хакер.-2008.-№10/08(118)

Поляков В.А. Методические указания к лабораторным работам по разделу «Безопасность обслуживания электроустановок» курса «Безопасность жизнедеятельности»/ Поляков В.А., Беленков Е.М., Завальнюк А.Ф., Аксенова И.В.-М.: МИЭМ,  2002.-20 с.

Облачные вычисления Oracle. Общий обзор, 5 мая 2013 г. // 

Client Hyper-V Survival Guide, 20 апреля 2013 г. // .

Облачные вычисления. Архитектура, 23 февраля 2013 г. // .

  Основные сведения о неоднородном доступе к памяти, 13 апреля 2013 г. // 

. 

  Н.Ромоданов. Перевод статьи М. Тим Джонс.“Анатомия Linux-гипервизора”,  15 апреля 2013 г. // . 

  Виртуализация и виртуальные машины, 10 марта 2013 г. // .

  Архитектура Hyper-V, 19 февраля 2013 г. // .

  Виртуализация. Оптимизация использования памяти Hyper-V, 9 марта 2013 г. // 

.

  NUMизматика, NUMерология и просто о NUMA, 11 мая 2013 г. // .

  Секреты виртуализации Hyper-V Windows Server 2012 от Экс Архитектора Microsoft Алексея Кибкало, 14 марта 2013 г. // http://habrahabr.ru/company/stars_s/blog/177175.

Эскиз архитектуры компонента Windows Server "8" Hyper-V, 25 марта 2013 г. // . 

Построение инфраструктуры облака: обзор сценария, 18 марта 2013 г. // .

Архитектура Hyper-V: Глубокое погружение, 11 АПРЕЛЯ 2013 г. // http://habrahabr.ru/post/98580.

Hyper-V и сети, 10 апреля 2013 г. // http://habrahabr.ru/post/97085.

Настройка виртуальных процессоров в Hyper-V, 5 марта 2013 г. // http://windowsnotes.ru/virtualization/nastrojka-virtualnyx-processorov-v-hyper-v.

Динамическая память в Hyper-V: принцип работы и настройка, 5 марта 2013 г. // .

Обзор служб удаленных рабочих столов, 1 мая 2013 г. // .

 Облачные технологии (Cloud Computing), 10 февраля 2013г. // 

Динамическая память в Hyper-V: принцип работы и настройка,7марта2013г.// 

 Заметки о Windows. Hyper-V Resourse Metering, 7 марта 2013 г. // 

Заметки о Windows. Настройка виртуальных процессоров в Hyper-V, 7 марта2013 г. // . 

 Роль программного обеспечения как услуг в облачных вычислениях. IBM, 7 марта 2013г. // 

 Блог компании Селектел. Об учете процессорного времени в облаке, 9 марта 2013 г. // 

Performance Monitor. В помощь системному администратору, 9 марта 2013 г. //

 

Заметки о Windows. PowerShell 3.0. Обзор новых возможностей (ч.2), 10 апреля 2013г. // 

 Hyper-V и производительность. Часть 1 -  Как тестировать?, 18 марта 2013 г. // 

Hyper-V и производительность. Часть 6 – расчет загрузки процессора виртуальными машинами при помощи WMI, 18 марта 2013 г. // 

Применение счетчиков производительности. Часть 4., 19 марта 2013г. //  

 Hyper-V и NUMA. Балансировка виртуальных машин в системах NUMA, 25 мая 2013 г. // 

Модели сервисов облачных вычислений. Часть 1. Инфраструктура как сервис., 30 мая 2013 г. // 

Облачные услуги. Снижение рисков и обеспечение высокой готовности. , 30 мая 2013 г. // 

Технология гипервизора. , 30 мая 2013 г.  // 

 Client Hyper-V, 19 марта 2013 г . // 

Нейл Такер. Создание виртуальных машин средствами Windows Power Shell, 20 марта 2013г. // 

Основы облачных вычислений, 5 марта 2013 г. // 

Гипервизоры, Виртуализация и Облако. О гипервизорах, виртуализации и о том, как это работает в облачной среде., 5 марта 2013 г . // 

Erik Scholten. Hypervisor  Comparision, 20 марта 2013г. //  Http://Vmguru.nl. 

Облачные вычисления, 1 апреля 2013г. // 

Гипервизор, 1 апреля 2013 г. //  

Виртуализация в Windows 8: встроенный Hyper-V, 30 мая 2013 г. //  

Жесткие диски в Microsoft Hyper-V, 30 мая 2013 г. // 

ГОСТ 27.002-89. Межгосударственный стандарт. Надежность в технике. Основные понятия. Термины и определения

ГОСТ 19.502-78 ЕСПД. Описание применения. Требования к содержанию и оформлению;

ГОСТ  12.1.030-81 Электробезопасность. Защитные заземления, зануления.

ГОСТ 7.0.5-2008. Библиографическая ссылка.




Федеральное государственное автономное образовательное учреждение

высшего профессионального образования







НАЦИОНАЛЬНЫЙ ИССЛЕДОВТЕЛЬСКИЙ УНИВЕРСИТЕТ

ВЫСШАЯ ШКОЛА ЭКОНОМИКИ



МОСКОВСКИЙ ИНСТИТУТ ЭЛЕКТРОНИКИ И МАТЕМАТИКИ

НАЦИОНАЛЬНОГО ИССЛЕДОВАТЕЛЬСКОГО УНИВЕРСИТЕТА

«ВЫСШАЯ ШКОЛА ЭКОНОМИКИ»





Графические материалы

к дипломному проекту

На тему :

«Организация работы гипервизора при создании систем облачных вычислений»

















Студент Дашков Михаил Михайлович



Руководитель проекта Саксонов Евгений Александрович

Приложение 1.






Приложение 2.






Приложение 3.






Приложение 4.




Приложение 5.






Приложение 6.






Приложение 7.






Приложение 8.

